{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional Generative Adversarial Network (DCGAN) Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorials walks through an implementation of DCGAN as described in [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434).\n",
    "\n",
    "To learn more about generative adversarial networks, see my [Medium post](https://medium.com/p/54deab2fce39) on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries we will need.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import scipy.misc\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the MNIST dataset. input_data is a library that downloads the dataset and uzips it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "     with tf.variable_scope(name):\n",
    "         f1 = 0.5 * (1 + leak)\n",
    "         f2 = 0.5 * (1 - leak)\n",
    "         return f1 * x + f2 * abs(x)\n",
    "    \n",
    "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
    "#They allow for saving sample images from the generator to follow progress\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def inverse_transform(images):\n",
    "    return (images+1.)/2.\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1]))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network\n",
    "\n",
    "The generator takes a vector of random numbers and transforms it into a 32x32 image. Each layer in the network involves a strided  transpose convolution, batch normalization, and rectified nonlinearity. Tensorflow's slim library allows us to easily define each of these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    \n",
    "    zP = slim.fully_connected(z,4*4*256,normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_project',weights_initializer=initializer)\n",
    "    zCon = tf.reshape(zP,[-1,4,4,256])\n",
    "    \n",
    "    gen1 = slim.convolution2d_transpose(\\\n",
    "        zCon,num_outputs=64,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
    "    \n",
    "    gen2 = slim.convolution2d_transpose(\\\n",
    "        gen1,num_outputs=32,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
    "    \n",
    "    gen3 = slim.convolution2d_transpose(\\\n",
    "        gen2,num_outputs=16,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv3', weights_initializer=initializer)\n",
    "    \n",
    "    g_out = slim.convolution2d_transpose(\\\n",
    "        gen3,num_outputs=1,kernel_size=[32,32],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
    "        scope='g_out', weights_initializer=initializer)\n",
    "    \n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network\n",
    "The discriminator network takes as input a 32x32 image and transforms it into a single valued probability of being generated from real-world data. Again we use tf.slim to define the convolutional layers, batch normalization, and weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(bottom, reuse=False):\n",
    "    \n",
    "    dis1 = slim.convolution2d(bottom,16,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv1',weights_initializer=initializer)\n",
    "    \n",
    "    dis2 = slim.convolution2d(dis1,32,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv2', weights_initializer=initializer)\n",
    "    \n",
    "    dis3 = slim.convolution2d(dis2,64,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv3',weights_initializer=initializer)\n",
    "    \n",
    "    d_out = slim.fully_connected(slim.flatten(dis3),1,activation_fn=tf.nn.sigmoid,\\\n",
    "        reuse=reuse,scope='d_out', weights_initializer=initializer)\n",
    "    \n",
    "    return d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "z_size = 100 #Size of z vector used for generator.\n",
    "\n",
    "#This initializaer is used to initialize all the weights of the network.\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "#These two placeholders are used for input into the generator and discriminator, respectively.\n",
    "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
    "real_in = tf.placeholder(shape=[None,32,32,1],dtype=tf.float32) #Real images\n",
    "\n",
    "Gz = generator(z_in) #Generates images from random z vectors\n",
    "Dx = discriminator(real_in) #Produces probabilities for real images\n",
    "Dg = discriminator(Gz,reuse=True) #Produces probabilities for generator images\n",
    "\n",
    "#These functions together define the optimization objective of the GAN.\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
    "g_loss = -tf.reduce_mean(tf.log(Dg)) #This optimizes the generator.\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "#The below code is responsible for applying gradient descent to update the GAN.\n",
    "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "trainerG = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "d_grads = trainerD.compute_gradients(d_loss,tvars[9:]) #Only update the weights for the discriminator network.\n",
    "g_grads = trainerG.compute_gradients(g_loss,tvars[0:9]) #Only update the weights for the generator network.\n",
    "\n",
    "update_D = trainerD.apply_gradients(d_grads)\n",
    "update_G = trainerG.apply_gradients(g_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the network\n",
    "Now that we have fully defined our network, it is time to train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 0.289768 Disc Loss: 1.55307\n",
      "Gen Loss: 0.21409 Disc Loss: 2.01614\n",
      "Gen Loss: 0.340757 Disc Loss: 1.95377\n",
      "Gen Loss: 0.535975 Disc Loss: 1.75118\n",
      "Gen Loss: 0.715503 Disc Loss: 1.52133\n",
      "Gen Loss: 0.588635 Disc Loss: 1.75493\n",
      "Gen Loss: 0.691759 Disc Loss: 1.49491\n",
      "Gen Loss: 0.826789 Disc Loss: 1.52939\n",
      "Gen Loss: 0.764096 Disc Loss: 1.49746\n",
      "Gen Loss: 0.854176 Disc Loss: 1.36628\n",
      "Gen Loss: 0.860045 Disc Loss: 1.33524\n",
      "Gen Loss: 0.74409 Disc Loss: 1.43806\n",
      "Gen Loss: 0.792733 Disc Loss: 1.399\n",
      "Gen Loss: 0.853702 Disc Loss: 1.51812\n",
      "Gen Loss: 0.855788 Disc Loss: 1.39641\n",
      "Gen Loss: 1.19717 Disc Loss: 1.06158\n",
      "Gen Loss: 0.5876 Disc Loss: 1.63713\n",
      "Gen Loss: 0.661258 Disc Loss: 1.4365\n",
      "Gen Loss: 1.20765 Disc Loss: 0.87197\n",
      "Gen Loss: 1.11869 Disc Loss: 1.02742\n",
      "Gen Loss: 0.434067 Disc Loss: 1.77069\n",
      "Gen Loss: 0.902206 Disc Loss: 1.2196\n",
      "Gen Loss: 0.979022 Disc Loss: 1.19168\n",
      "Gen Loss: 2.17373 Disc Loss: 0.337141\n",
      "Gen Loss: 2.56994 Disc Loss: 0.221819\n",
      "Gen Loss: 0.514688 Disc Loss: 2.36981\n",
      "Gen Loss: 1.06086 Disc Loss: 1.35439\n",
      "Gen Loss: 0.96115 Disc Loss: 1.3447\n",
      "Gen Loss: 0.959453 Disc Loss: 1.15505\n",
      "Gen Loss: 1.39501 Disc Loss: 0.917492\n",
      "Gen Loss: 0.783652 Disc Loss: 0.965958\n",
      "Gen Loss: 1.11115 Disc Loss: 0.950955\n",
      "Gen Loss: 0.976835 Disc Loss: 1.52264\n",
      "Gen Loss: 0.974672 Disc Loss: 1.38694\n",
      "Gen Loss: 0.66224 Disc Loss: 1.35761\n",
      "Gen Loss: 1.19668 Disc Loss: 1.06089\n",
      "Gen Loss: 1.00118 Disc Loss: 1.35972\n",
      "Gen Loss: 0.739007 Disc Loss: 1.29669\n",
      "Gen Loss: 1.02852 Disc Loss: 1.17434\n",
      "Gen Loss: 1.18392 Disc Loss: 1.24696\n",
      "Gen Loss: 0.901333 Disc Loss: 1.19805\n",
      "Gen Loss: 1.21378 Disc Loss: 0.968979\n",
      "Gen Loss: 0.959796 Disc Loss: 1.36249\n",
      "Gen Loss: 1.31542 Disc Loss: 0.958755\n",
      "Gen Loss: 0.924309 Disc Loss: 1.43438\n",
      "Gen Loss: 0.933795 Disc Loss: 1.24716\n",
      "Gen Loss: 1.48876 Disc Loss: 0.827086\n",
      "Gen Loss: 0.908424 Disc Loss: 1.1163\n",
      "Gen Loss: 0.805212 Disc Loss: 1.36367\n",
      "Gen Loss: 0.845105 Disc Loss: 1.00595\n",
      "Gen Loss: 1.02331 Disc Loss: 1.16274\n",
      "Gen Loss: 1.19254 Disc Loss: 0.860454\n",
      "Gen Loss: 1.11009 Disc Loss: 0.995796\n",
      "Gen Loss: 1.11099 Disc Loss: 0.899848\n",
      "Gen Loss: 1.237 Disc Loss: 0.872682\n",
      "Gen Loss: 1.56509 Disc Loss: 0.698804\n",
      "Gen Loss: 1.37299 Disc Loss: 0.857864\n",
      "Gen Loss: 0.883207 Disc Loss: 0.988147\n",
      "Gen Loss: 1.10063 Disc Loss: 1.15317\n",
      "Gen Loss: 1.25636 Disc Loss: 0.94631\n",
      "Gen Loss: 1.10726 Disc Loss: 1.29057\n",
      "Gen Loss: 1.61168 Disc Loss: 0.849813\n",
      "Gen Loss: 1.39029 Disc Loss: 1.14859\n",
      "Gen Loss: 0.912499 Disc Loss: 1.29198\n",
      "Gen Loss: 1.19243 Disc Loss: 1.05731\n",
      "Gen Loss: 1.12303 Disc Loss: 0.96269\n",
      "Gen Loss: 1.04645 Disc Loss: 1.1993\n",
      "Gen Loss: 1.19545 Disc Loss: 1.02972\n",
      "Gen Loss: 0.954871 Disc Loss: 1.40563\n",
      "Gen Loss: 1.22539 Disc Loss: 0.851192\n",
      "Gen Loss: 1.19261 Disc Loss: 1.14774\n",
      "Gen Loss: 0.94365 Disc Loss: 1.41103\n",
      "Gen Loss: 0.884823 Disc Loss: 1.34489\n",
      "Gen Loss: 0.976262 Disc Loss: 0.954125\n",
      "Gen Loss: 0.799651 Disc Loss: 1.50812\n",
      "Gen Loss: 0.820454 Disc Loss: 1.31566\n",
      "Gen Loss: 0.769849 Disc Loss: 1.38828\n",
      "Gen Loss: 0.800012 Disc Loss: 1.26287\n",
      "Gen Loss: 0.83042 Disc Loss: 1.36648\n",
      "Gen Loss: 0.933835 Disc Loss: 1.04239\n",
      "Gen Loss: 0.96155 Disc Loss: 1.29925\n",
      "Gen Loss: 0.968082 Disc Loss: 0.98518\n",
      "Gen Loss: 1.16709 Disc Loss: 1.0754\n",
      "Gen Loss: 0.997532 Disc Loss: 1.23918\n",
      "Gen Loss: 0.921988 Disc Loss: 1.1472\n",
      "Gen Loss: 0.837325 Disc Loss: 1.29125\n",
      "Gen Loss: 0.732883 Disc Loss: 1.24951\n",
      "Gen Loss: 0.941247 Disc Loss: 1.13829\n",
      "Gen Loss: 0.764447 Disc Loss: 1.1723\n",
      "Gen Loss: 0.994707 Disc Loss: 1.19133\n",
      "Gen Loss: 0.815403 Disc Loss: 1.4201\n",
      "Gen Loss: 0.836789 Disc Loss: 1.16326\n",
      "Gen Loss: 0.993347 Disc Loss: 1.18644\n",
      "Gen Loss: 0.920665 Disc Loss: 1.26095\n",
      "Gen Loss: 0.947126 Disc Loss: 1.08415\n",
      "Gen Loss: 0.770756 Disc Loss: 1.35249\n",
      "Gen Loss: 0.820019 Disc Loss: 1.19449\n",
      "Gen Loss: 0.900543 Disc Loss: 1.13117\n",
      "Gen Loss: 0.96663 Disc Loss: 1.07686\n",
      "Gen Loss: 1.00731 Disc Loss: 1.21149\n",
      "Gen Loss: 1.03269 Disc Loss: 1.03244\n",
      "Saved Model\n",
      "Gen Loss: 1.04818 Disc Loss: 1.48598\n",
      "Gen Loss: 1.02544 Disc Loss: 0.97309\n",
      "Gen Loss: 0.88951 Disc Loss: 1.1248\n",
      "Gen Loss: 0.889997 Disc Loss: 1.08171\n",
      "Gen Loss: 0.936044 Disc Loss: 1.23521\n",
      "Gen Loss: 1.14631 Disc Loss: 0.927616\n",
      "Gen Loss: 1.05366 Disc Loss: 1.03397\n",
      "Gen Loss: 0.849266 Disc Loss: 1.36924\n",
      "Gen Loss: 1.10556 Disc Loss: 1.09334\n",
      "Gen Loss: 1.09616 Disc Loss: 1.27006\n",
      "Gen Loss: 0.841461 Disc Loss: 1.46759\n",
      "Gen Loss: 0.755742 Disc Loss: 1.34488\n",
      "Gen Loss: 0.90588 Disc Loss: 1.21166\n",
      "Gen Loss: 0.854324 Disc Loss: 1.20839\n",
      "Gen Loss: 1.01586 Disc Loss: 1.33705\n",
      "Gen Loss: 1.08099 Disc Loss: 1.50645\n",
      "Gen Loss: 1.03246 Disc Loss: 0.970935\n",
      "Gen Loss: 1.03888 Disc Loss: 1.23665\n",
      "Gen Loss: 1.07815 Disc Loss: 1.10647\n",
      "Gen Loss: 1.00189 Disc Loss: 0.912043\n",
      "Gen Loss: 1.12887 Disc Loss: 1.16536\n",
      "Gen Loss: 1.25153 Disc Loss: 0.739688\n",
      "Gen Loss: 1.21015 Disc Loss: 1.10558\n",
      "Gen Loss: 1.15793 Disc Loss: 0.78266\n",
      "Gen Loss: 0.901715 Disc Loss: 1.24789\n",
      "Gen Loss: 0.960117 Disc Loss: 1.46884\n",
      "Gen Loss: 0.976091 Disc Loss: 1.07142\n",
      "Gen Loss: 1.61967 Disc Loss: 0.538355\n",
      "Gen Loss: 1.196 Disc Loss: 0.980978\n",
      "Gen Loss: 1.47678 Disc Loss: 0.695366\n",
      "Gen Loss: 0.690336 Disc Loss: 1.43498\n",
      "Gen Loss: 1.42709 Disc Loss: 0.817808\n",
      "Gen Loss: 1.12406 Disc Loss: 1.13738\n",
      "Gen Loss: 1.66276 Disc Loss: 0.529543\n",
      "Gen Loss: 1.18239 Disc Loss: 0.997509\n",
      "Gen Loss: 1.40739 Disc Loss: 0.807957\n",
      "Gen Loss: 1.06852 Disc Loss: 0.868327\n",
      "Gen Loss: 0.999901 Disc Loss: 1.00616\n",
      "Gen Loss: 0.958038 Disc Loss: 1.19491\n",
      "Gen Loss: 1.15452 Disc Loss: 1.11834\n",
      "Gen Loss: 1.31953 Disc Loss: 0.85744\n",
      "Gen Loss: 1.08707 Disc Loss: 1.1199\n",
      "Gen Loss: 1.04093 Disc Loss: 0.99511\n",
      "Gen Loss: 1.15224 Disc Loss: 0.957164\n",
      "Gen Loss: 1.24525 Disc Loss: 0.997854\n",
      "Gen Loss: 1.18963 Disc Loss: 0.929694\n",
      "Gen Loss: 0.699293 Disc Loss: 1.50269\n",
      "Gen Loss: 1.16517 Disc Loss: 0.828997\n",
      "Gen Loss: 0.884726 Disc Loss: 1.15302\n",
      "Gen Loss: 1.00669 Disc Loss: 0.996081\n",
      "Gen Loss: 0.887435 Disc Loss: 1.16992\n",
      "Gen Loss: 0.966904 Disc Loss: 0.995947\n",
      "Gen Loss: 0.901785 Disc Loss: 1.25482\n",
      "Gen Loss: 1.32256 Disc Loss: 1.07905\n",
      "Gen Loss: 1.72995 Disc Loss: 0.488298\n",
      "Gen Loss: 1.57422 Disc Loss: 0.699388\n",
      "Gen Loss: 0.960567 Disc Loss: 1.18592\n",
      "Gen Loss: 1.33367 Disc Loss: 0.871544\n",
      "Gen Loss: 1.08863 Disc Loss: 0.7517\n",
      "Gen Loss: 1.15718 Disc Loss: 0.97256\n",
      "Gen Loss: 1.15229 Disc Loss: 0.907267\n",
      "Gen Loss: 1.18742 Disc Loss: 0.854538\n",
      "Gen Loss: 1.14467 Disc Loss: 1.14472\n",
      "Gen Loss: 0.761541 Disc Loss: 1.42839\n",
      "Gen Loss: 1.63205 Disc Loss: 1.1759\n",
      "Gen Loss: 0.943071 Disc Loss: 1.20062\n",
      "Gen Loss: 1.80081 Disc Loss: 0.974544\n",
      "Gen Loss: 1.83585 Disc Loss: 0.553447\n",
      "Gen Loss: 1.15124 Disc Loss: 1.07027\n",
      "Gen Loss: 1.41923 Disc Loss: 0.793075\n",
      "Gen Loss: 1.46063 Disc Loss: 0.694792\n",
      "Gen Loss: 1.39156 Disc Loss: 0.766736\n",
      "Gen Loss: 1.79131 Disc Loss: 0.842661\n",
      "Gen Loss: 0.881496 Disc Loss: 1.15272\n",
      "Gen Loss: 1.22249 Disc Loss: 0.96586\n",
      "Gen Loss: 1.33862 Disc Loss: 0.660941\n",
      "Gen Loss: 1.16148 Disc Loss: 1.02154\n",
      "Gen Loss: 1.13186 Disc Loss: 1.0413\n",
      "Gen Loss: 1.22779 Disc Loss: 0.911577\n",
      "Gen Loss: 1.23316 Disc Loss: 0.862326\n",
      "Gen Loss: 1.43792 Disc Loss: 0.774333\n",
      "Gen Loss: 1.52166 Disc Loss: 0.71541\n",
      "Gen Loss: 1.0521 Disc Loss: 1.12702\n",
      "Gen Loss: 1.4293 Disc Loss: 0.800989\n",
      "Gen Loss: 0.944615 Disc Loss: 0.92889\n",
      "Gen Loss: 1.6989 Disc Loss: 0.512526\n",
      "Gen Loss: 1.30519 Disc Loss: 0.8743\n",
      "Gen Loss: 1.12889 Disc Loss: 0.931726\n",
      "Gen Loss: 1.34988 Disc Loss: 0.809022\n",
      "Gen Loss: 1.97912 Disc Loss: 0.796083\n",
      "Gen Loss: 1.00694 Disc Loss: 1.12031\n",
      "Gen Loss: 1.76815 Disc Loss: 0.486344\n",
      "Gen Loss: 1.23528 Disc Loss: 0.965095\n",
      "Gen Loss: 1.47183 Disc Loss: 0.820475\n",
      "Gen Loss: 0.747634 Disc Loss: 1.20724\n",
      "Gen Loss: 1.44461 Disc Loss: 0.65637\n",
      "Gen Loss: 1.56011 Disc Loss: 0.641623\n",
      "Gen Loss: 1.32653 Disc Loss: 0.834328\n",
      "Gen Loss: 1.56747 Disc Loss: 0.818343\n",
      "Gen Loss: 0.702537 Disc Loss: 1.3422\n",
      "Saved Model\n",
      "Gen Loss: 1.20744 Disc Loss: 1.19762\n",
      "Gen Loss: 1.04265 Disc Loss: 1.06771\n",
      "Gen Loss: 0.829941 Disc Loss: 1.23396\n",
      "Gen Loss: 1.46547 Disc Loss: 0.79057\n",
      "Gen Loss: 1.37229 Disc Loss: 0.994914\n",
      "Gen Loss: 1.64217 Disc Loss: 0.671894\n",
      "Gen Loss: 1.82686 Disc Loss: 0.79671\n",
      "Gen Loss: 1.12696 Disc Loss: 0.983295\n",
      "Gen Loss: 1.21004 Disc Loss: 0.746708\n",
      "Gen Loss: 0.956357 Disc Loss: 1.0405\n",
      "Gen Loss: 1.58267 Disc Loss: 0.493704\n",
      "Gen Loss: 1.24899 Disc Loss: 0.873128\n",
      "Gen Loss: 1.28177 Disc Loss: 0.808631\n",
      "Gen Loss: 0.917293 Disc Loss: 1.11803\n",
      "Gen Loss: 1.2156 Disc Loss: 0.817995\n",
      "Gen Loss: 1.2755 Disc Loss: 1.14198\n",
      "Gen Loss: 1.13104 Disc Loss: 0.698504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 1.10375 Disc Loss: 0.975712\n",
      "Gen Loss: 1.27683 Disc Loss: 0.881377\n",
      "Gen Loss: 1.19909 Disc Loss: 1.00015\n",
      "Gen Loss: 1.61747 Disc Loss: 0.657199\n",
      "Gen Loss: 1.38317 Disc Loss: 0.986403\n",
      "Gen Loss: 0.94899 Disc Loss: 1.27113\n",
      "Gen Loss: 2.1342 Disc Loss: 0.340827\n",
      "Gen Loss: 1.93802 Disc Loss: 0.630129\n",
      "Gen Loss: 1.62913 Disc Loss: 0.656727\n",
      "Gen Loss: 1.45516 Disc Loss: 0.718656\n",
      "Gen Loss: 1.74658 Disc Loss: 0.843336\n",
      "Gen Loss: 1.25731 Disc Loss: 0.772681\n",
      "Gen Loss: 1.06081 Disc Loss: 1.49185\n",
      "Gen Loss: 1.20097 Disc Loss: 0.901113\n",
      "Gen Loss: 1.13506 Disc Loss: 1.12579\n",
      "Gen Loss: 1.05336 Disc Loss: 1.08942\n",
      "Gen Loss: 0.887609 Disc Loss: 1.16804\n",
      "Gen Loss: 1.7222 Disc Loss: 0.799442\n",
      "Gen Loss: 1.13345 Disc Loss: 1.10965\n",
      "Gen Loss: 1.48108 Disc Loss: 0.78666\n",
      "Gen Loss: 1.66678 Disc Loss: 0.594802\n",
      "Gen Loss: 1.73011 Disc Loss: 0.815831\n",
      "Gen Loss: 1.23964 Disc Loss: 0.829121\n",
      "Gen Loss: 1.07707 Disc Loss: 0.753547\n",
      "Gen Loss: 1.38097 Disc Loss: 1.4034\n",
      "Gen Loss: 1.49178 Disc Loss: 0.860231\n",
      "Gen Loss: 1.42902 Disc Loss: 1.28055\n",
      "Gen Loss: 1.92135 Disc Loss: 0.475377\n",
      "Gen Loss: 1.98887 Disc Loss: 0.382405\n",
      "Gen Loss: 1.65329 Disc Loss: 0.751227\n",
      "Gen Loss: 1.64495 Disc Loss: 0.496936\n",
      "Gen Loss: 1.82846 Disc Loss: 0.464185\n",
      "Gen Loss: 1.48877 Disc Loss: 0.593276\n",
      "Gen Loss: 1.38211 Disc Loss: 0.685912\n",
      "Gen Loss: 2.34469 Disc Loss: 0.809541\n",
      "Gen Loss: 1.72512 Disc Loss: 0.400244\n",
      "Gen Loss: 1.52113 Disc Loss: 1.33835\n",
      "Gen Loss: 1.04874 Disc Loss: 1.10441\n",
      "Gen Loss: 1.19364 Disc Loss: 1.13174\n",
      "Gen Loss: 1.80802 Disc Loss: 0.477242\n",
      "Gen Loss: 1.30152 Disc Loss: 1.19854\n",
      "Gen Loss: 1.60228 Disc Loss: 0.745178\n",
      "Gen Loss: 1.77908 Disc Loss: 0.923337\n",
      "Gen Loss: 0.880383 Disc Loss: 0.431346\n",
      "Gen Loss: 1.62071 Disc Loss: 0.914482\n",
      "Gen Loss: 1.30398 Disc Loss: 0.656406\n",
      "Gen Loss: 2.28141 Disc Loss: 0.72683\n",
      "Gen Loss: 1.89846 Disc Loss: 0.493246\n",
      "Gen Loss: 1.90284 Disc Loss: 0.516851\n",
      "Gen Loss: 1.23909 Disc Loss: 0.89832\n",
      "Gen Loss: 2.11843 Disc Loss: 0.766034\n",
      "Gen Loss: 1.24785 Disc Loss: 0.948807\n",
      "Gen Loss: 1.92859 Disc Loss: 0.61746\n",
      "Gen Loss: 2.06726 Disc Loss: 0.54443\n",
      "Gen Loss: 1.28817 Disc Loss: 0.731363\n",
      "Gen Loss: 1.27624 Disc Loss: 0.823478\n",
      "Gen Loss: 1.77418 Disc Loss: 0.743625\n",
      "Gen Loss: 1.57948 Disc Loss: 0.632251\n",
      "Gen Loss: 1.57198 Disc Loss: 0.711634\n",
      "Gen Loss: 1.74505 Disc Loss: 0.552136\n",
      "Gen Loss: 1.96211 Disc Loss: 0.489885\n",
      "Gen Loss: 1.77734 Disc Loss: 0.64151\n",
      "Gen Loss: 2.07551 Disc Loss: 0.638606\n",
      "Gen Loss: 1.58482 Disc Loss: 0.743997\n",
      "Gen Loss: 1.13862 Disc Loss: 1.00896\n",
      "Gen Loss: 1.82199 Disc Loss: 0.888474\n",
      "Gen Loss: 1.6131 Disc Loss: 0.479574\n",
      "Gen Loss: 1.83495 Disc Loss: 0.727638\n",
      "Gen Loss: 1.87915 Disc Loss: 0.854898\n",
      "Gen Loss: 2.18562 Disc Loss: 0.373465\n",
      "Gen Loss: 1.64781 Disc Loss: 0.597103\n",
      "Gen Loss: 1.3135 Disc Loss: 0.813156\n",
      "Gen Loss: 0.908303 Disc Loss: 1.47858\n",
      "Gen Loss: 1.5248 Disc Loss: 0.712575\n",
      "Gen Loss: 1.7607 Disc Loss: 0.606969\n",
      "Gen Loss: 2.15434 Disc Loss: 0.370898\n",
      "Gen Loss: 2.03935 Disc Loss: 0.689704\n",
      "Gen Loss: 1.81539 Disc Loss: 0.60195\n",
      "Gen Loss: 1.96597 Disc Loss: 0.49\n",
      "Gen Loss: 1.97843 Disc Loss: 0.286115\n",
      "Gen Loss: 1.4491 Disc Loss: 0.50951\n",
      "Gen Loss: 1.92689 Disc Loss: 0.38813\n",
      "Gen Loss: 1.37757 Disc Loss: 0.536302\n",
      "Saved Model\n",
      "Gen Loss: 1.42391 Disc Loss: 0.880997\n",
      "Gen Loss: 1.47662 Disc Loss: 0.536393\n",
      "Gen Loss: 1.79256 Disc Loss: 0.660037\n",
      "Gen Loss: 1.52646 Disc Loss: 0.515489\n",
      "Gen Loss: 1.83661 Disc Loss: 0.554051\n",
      "Gen Loss: 2.30221 Disc Loss: 0.301477\n",
      "Gen Loss: 2.60956 Disc Loss: 0.41722\n",
      "Gen Loss: 2.34024 Disc Loss: 0.225161\n",
      "Gen Loss: 2.24176 Disc Loss: 0.578959\n",
      "Gen Loss: 0.942978 Disc Loss: 1.23919\n",
      "Gen Loss: 1.55445 Disc Loss: 0.763124\n",
      "Gen Loss: 2.02205 Disc Loss: 0.280448\n",
      "Gen Loss: 1.19227 Disc Loss: 0.927248\n",
      "Gen Loss: 1.51031 Disc Loss: 0.772588\n",
      "Gen Loss: 2.01538 Disc Loss: 0.696773\n",
      "Gen Loss: 2.65269 Disc Loss: 0.506564\n",
      "Gen Loss: 2.37973 Disc Loss: 0.334198\n",
      "Gen Loss: 1.78028 Disc Loss: 0.836129\n",
      "Gen Loss: 2.13299 Disc Loss: 0.322983\n",
      "Gen Loss: 1.72223 Disc Loss: 0.491426\n",
      "Gen Loss: 2.10825 Disc Loss: 0.375949\n",
      "Gen Loss: 1.37934 Disc Loss: 0.69678\n",
      "Gen Loss: 1.44202 Disc Loss: 0.929205\n",
      "Gen Loss: 0.750911 Disc Loss: 1.00322\n",
      "Gen Loss: 1.96882 Disc Loss: 0.363848\n",
      "Gen Loss: 1.35102 Disc Loss: 0.83384\n",
      "Gen Loss: 1.19089 Disc Loss: 0.70512\n",
      "Gen Loss: 2.08976 Disc Loss: 0.587331\n",
      "Gen Loss: 0.621659 Disc Loss: 1.47015\n",
      "Gen Loss: 1.35233 Disc Loss: 1.15857\n",
      "Gen Loss: 1.37871 Disc Loss: 0.498657\n",
      "Gen Loss: 1.4289 Disc Loss: 0.420949\n",
      "Gen Loss: 2.80681 Disc Loss: 0.782118\n",
      "Gen Loss: 2.03637 Disc Loss: 0.311643\n",
      "Gen Loss: 2.18683 Disc Loss: 0.694075\n",
      "Gen Loss: 1.97315 Disc Loss: 0.72191\n",
      "Gen Loss: 1.66418 Disc Loss: 0.560757\n",
      "Gen Loss: 2.55098 Disc Loss: 0.482746\n",
      "Gen Loss: 1.42411 Disc Loss: 0.576636\n",
      "Gen Loss: 2.11908 Disc Loss: 0.359221\n",
      "Gen Loss: 2.57599 Disc Loss: 0.574351\n",
      "Gen Loss: 1.70895 Disc Loss: 0.740118\n",
      "Gen Loss: 1.19513 Disc Loss: 0.827181\n",
      "Gen Loss: 1.98719 Disc Loss: 0.360904\n",
      "Gen Loss: 1.99423 Disc Loss: 0.496325\n",
      "Gen Loss: 1.31638 Disc Loss: 0.708791\n",
      "Gen Loss: 2.1949 Disc Loss: 0.404245\n",
      "Gen Loss: 1.87129 Disc Loss: 0.278902\n",
      "Gen Loss: 1.96732 Disc Loss: 0.471742\n",
      "Gen Loss: 1.10479 Disc Loss: 0.976515\n",
      "Gen Loss: 2.01152 Disc Loss: 0.652702\n",
      "Gen Loss: 2.00477 Disc Loss: 0.47708\n",
      "Gen Loss: 2.0617 Disc Loss: 0.336456\n",
      "Gen Loss: 1.90457 Disc Loss: 0.337801\n",
      "Gen Loss: 1.63196 Disc Loss: 0.618321\n",
      "Gen Loss: 1.98432 Disc Loss: 0.319473\n",
      "Gen Loss: 0.805431 Disc Loss: 0.938285\n",
      "Gen Loss: 0.727159 Disc Loss: 0.781704\n",
      "Gen Loss: 1.21613 Disc Loss: 0.872633\n",
      "Gen Loss: 1.87083 Disc Loss: 0.392692\n",
      "Gen Loss: 1.85546 Disc Loss: 0.294577\n",
      "Gen Loss: 1.93528 Disc Loss: 0.313597\n",
      "Gen Loss: 1.50408 Disc Loss: 0.562232\n",
      "Gen Loss: 1.4419 Disc Loss: 0.603564\n",
      "Gen Loss: 2.03337 Disc Loss: 0.476901\n",
      "Gen Loss: 1.52847 Disc Loss: 0.473836\n",
      "Gen Loss: 1.5409 Disc Loss: 0.424582\n",
      "Gen Loss: 2.28689 Disc Loss: 0.287955\n",
      "Gen Loss: 1.67842 Disc Loss: 0.369676\n",
      "Gen Loss: 1.76596 Disc Loss: 0.511513\n",
      "Gen Loss: 2.15171 Disc Loss: 0.249664\n",
      "Gen Loss: 1.57002 Disc Loss: 0.505173\n",
      "Gen Loss: 1.81128 Disc Loss: 0.455026\n",
      "Gen Loss: 2.00458 Disc Loss: 0.462957\n",
      "Gen Loss: 2.24821 Disc Loss: 0.295029\n",
      "Gen Loss: 1.49851 Disc Loss: 0.304936\n",
      "Gen Loss: 1.65604 Disc Loss: 0.693447\n",
      "Gen Loss: 2.17346 Disc Loss: 1.20973\n",
      "Gen Loss: 2.20773 Disc Loss: 0.542473\n",
      "Gen Loss: 1.9965 Disc Loss: 0.634727\n",
      "Gen Loss: 1.93803 Disc Loss: 0.8895\n",
      "Gen Loss: 1.67557 Disc Loss: 0.68093\n",
      "Gen Loss: 1.57124 Disc Loss: 0.702659\n",
      "Gen Loss: 1.83919 Disc Loss: 0.480365\n",
      "Gen Loss: 1.95025 Disc Loss: 0.324065\n",
      "Gen Loss: 1.72306 Disc Loss: 1.07385\n",
      "Gen Loss: 2.37719 Disc Loss: 0.442906\n",
      "Gen Loss: 2.14505 Disc Loss: 0.540099\n",
      "Gen Loss: 1.90057 Disc Loss: 0.545557\n",
      "Gen Loss: 2.02956 Disc Loss: 0.293164\n",
      "Gen Loss: 1.70195 Disc Loss: 0.419504\n",
      "Gen Loss: 2.5922 Disc Loss: 0.222031\n",
      "Gen Loss: 1.52538 Disc Loss: 1.32863\n",
      "Gen Loss: 1.86507 Disc Loss: 0.39482\n",
      "Gen Loss: 2.23817 Disc Loss: 0.39043\n",
      "Gen Loss: 1.68406 Disc Loss: 0.697921\n",
      "Gen Loss: 1.70464 Disc Loss: 0.460351\n",
      "Gen Loss: 1.51688 Disc Loss: 0.515932\n",
      "Gen Loss: 1.35536 Disc Loss: 0.729628\n",
      "Gen Loss: 1.58037 Disc Loss: 0.999898\n",
      "Saved Model\n",
      "Gen Loss: 1.52506 Disc Loss: 0.494134\n",
      "Gen Loss: 1.21027 Disc Loss: 0.648802\n",
      "Gen Loss: 1.9506 Disc Loss: 0.637572\n",
      "Gen Loss: 1.76338 Disc Loss: 0.440681\n",
      "Gen Loss: 1.74424 Disc Loss: 0.45345\n",
      "Gen Loss: 1.37664 Disc Loss: 0.595628\n",
      "Gen Loss: 1.67755 Disc Loss: 0.494099\n",
      "Gen Loss: 2.90508 Disc Loss: 0.756972\n",
      "Gen Loss: 2.08323 Disc Loss: 0.610542\n",
      "Gen Loss: 1.59396 Disc Loss: 0.582556\n",
      "Gen Loss: 1.69331 Disc Loss: 0.557165\n",
      "Gen Loss: 1.82948 Disc Loss: 0.561484\n",
      "Gen Loss: 1.68267 Disc Loss: 0.658354\n",
      "Gen Loss: 1.88813 Disc Loss: 0.636121\n",
      "Gen Loss: 1.76949 Disc Loss: 0.424853\n",
      "Gen Loss: 1.45091 Disc Loss: 0.707211\n",
      "Gen Loss: 2.38915 Disc Loss: 0.427854\n",
      "Gen Loss: 0.968131 Disc Loss: 1.03656\n",
      "Gen Loss: 2.02168 Disc Loss: 0.333196\n",
      "Gen Loss: 1.91659 Disc Loss: 0.863487\n",
      "Gen Loss: 2.29621 Disc Loss: 0.392884\n",
      "Gen Loss: 1.88074 Disc Loss: 0.651659\n",
      "Gen Loss: 1.63431 Disc Loss: 0.707873\n",
      "Gen Loss: 1.98303 Disc Loss: 0.826545\n",
      "Gen Loss: 1.60853 Disc Loss: 0.736919\n",
      "Gen Loss: 1.42813 Disc Loss: 0.686701\n",
      "Gen Loss: 2.18477 Disc Loss: 0.376154\n",
      "Gen Loss: 2.23464 Disc Loss: 0.36188\n",
      "Gen Loss: 1.99787 Disc Loss: 0.3541\n",
      "Gen Loss: 1.50064 Disc Loss: 0.711455\n",
      "Gen Loss: 2.08869 Disc Loss: 0.369056\n",
      "Gen Loss: 2.02028 Disc Loss: 0.383058\n",
      "Gen Loss: 2.38011 Disc Loss: 0.360125\n",
      "Gen Loss: 1.54689 Disc Loss: 0.524377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 1.05857 Disc Loss: 0.748657\n",
      "Gen Loss: 1.42935 Disc Loss: 0.918655\n",
      "Gen Loss: 1.81839 Disc Loss: 0.540166\n",
      "Gen Loss: 2.00082 Disc Loss: 0.724719\n",
      "Gen Loss: 2.17855 Disc Loss: 0.220988\n",
      "Gen Loss: 2.31912 Disc Loss: 0.35049\n",
      "Gen Loss: 2.61572 Disc Loss: 0.444468\n",
      "Gen Loss: 1.75806 Disc Loss: 0.316903\n",
      "Gen Loss: 2.26837 Disc Loss: 0.383952\n",
      "Gen Loss: 2.11982 Disc Loss: 0.379381\n",
      "Gen Loss: 2.06506 Disc Loss: 0.325948\n",
      "Gen Loss: 2.34672 Disc Loss: 0.316264\n",
      "Gen Loss: 2.14885 Disc Loss: 0.558686\n",
      "Gen Loss: 2.11235 Disc Loss: 0.425544\n",
      "Gen Loss: 1.81234 Disc Loss: 0.437848\n",
      "Gen Loss: 1.48597 Disc Loss: 0.698689\n",
      "Gen Loss: 1.98935 Disc Loss: 0.601121\n",
      "Gen Loss: 1.11875 Disc Loss: 0.895433\n",
      "Gen Loss: 2.00174 Disc Loss: 0.581985\n",
      "Gen Loss: 1.8195 Disc Loss: 0.475083\n",
      "Gen Loss: 1.22013 Disc Loss: 0.808609\n",
      "Gen Loss: 1.77555 Disc Loss: 0.500855\n",
      "Gen Loss: 1.59589 Disc Loss: 0.461303\n",
      "Gen Loss: 2.04781 Disc Loss: 0.519144\n",
      "Gen Loss: 2.14639 Disc Loss: 0.416748\n",
      "Gen Loss: 3.22642 Disc Loss: 0.271229\n",
      "Gen Loss: 1.40405 Disc Loss: 0.629481\n",
      "Gen Loss: 1.82946 Disc Loss: 0.630512\n",
      "Gen Loss: 1.69473 Disc Loss: 0.37641\n",
      "Gen Loss: 3.1563 Disc Loss: 0.280437\n",
      "Gen Loss: 2.39606 Disc Loss: 0.421767\n",
      "Gen Loss: 2.17759 Disc Loss: 0.191893\n",
      "Gen Loss: 1.63257 Disc Loss: 0.819683\n",
      "Gen Loss: 0.773242 Disc Loss: 1.17177\n",
      "Gen Loss: 1.77162 Disc Loss: 0.627245\n",
      "Gen Loss: 0.856205 Disc Loss: 0.958602\n",
      "Gen Loss: 1.25878 Disc Loss: 0.760713\n",
      "Gen Loss: 1.70622 Disc Loss: 0.627152\n",
      "Gen Loss: 2.01124 Disc Loss: 0.520424\n",
      "Gen Loss: 1.58179 Disc Loss: 0.719437\n",
      "Gen Loss: 1.29117 Disc Loss: 1.55134\n",
      "Gen Loss: 1.93963 Disc Loss: 0.809782\n",
      "Gen Loss: 2.27911 Disc Loss: 0.61871\n",
      "Gen Loss: 2.20105 Disc Loss: 0.491532\n",
      "Gen Loss: 2.05079 Disc Loss: 0.298271\n",
      "Gen Loss: 1.57359 Disc Loss: 0.686283\n",
      "Gen Loss: 1.67639 Disc Loss: 0.626767\n",
      "Gen Loss: 2.09992 Disc Loss: 0.447428\n",
      "Gen Loss: 2.06452 Disc Loss: 0.471199\n",
      "Gen Loss: 2.21197 Disc Loss: 0.522201\n",
      "Gen Loss: 1.89519 Disc Loss: 0.35405\n",
      "Gen Loss: 1.26925 Disc Loss: 0.719752\n",
      "Gen Loss: 2.56648 Disc Loss: 0.519769\n",
      "Gen Loss: 2.50289 Disc Loss: 0.548418\n",
      "Gen Loss: 2.05274 Disc Loss: 0.375853\n",
      "Gen Loss: 1.59665 Disc Loss: 0.572638\n",
      "Gen Loss: 2.48623 Disc Loss: 0.349317\n",
      "Gen Loss: 2.35307 Disc Loss: 0.383599\n",
      "Gen Loss: 1.07621 Disc Loss: 0.875534\n",
      "Gen Loss: 1.86701 Disc Loss: 0.396443\n",
      "Gen Loss: 1.1469 Disc Loss: 1.08223\n",
      "Gen Loss: 1.46894 Disc Loss: 0.786275\n",
      "Gen Loss: 2.32953 Disc Loss: 0.257446\n",
      "Gen Loss: 1.73306 Disc Loss: 0.691514\n",
      "Gen Loss: 3.01676 Disc Loss: 1.01086\n",
      "Gen Loss: 1.4529 Disc Loss: 0.686532\n",
      "Saved Model\n",
      "Gen Loss: 2.8097 Disc Loss: 0.170765\n",
      "Gen Loss: 1.96633 Disc Loss: 0.570017\n",
      "Gen Loss: 1.84757 Disc Loss: 0.659763\n",
      "Gen Loss: 2.35164 Disc Loss: 0.361812\n",
      "Gen Loss: 1.56674 Disc Loss: 0.689081\n",
      "Gen Loss: 1.60683 Disc Loss: 0.610836\n",
      "Gen Loss: 1.28202 Disc Loss: 0.626819\n",
      "Gen Loss: 1.78977 Disc Loss: 0.418144\n",
      "Gen Loss: 1.35457 Disc Loss: 0.539909\n",
      "Gen Loss: 1.28143 Disc Loss: 0.716421\n",
      "Gen Loss: 1.90362 Disc Loss: 0.819417\n",
      "Gen Loss: 1.4555 Disc Loss: 0.644035\n",
      "Gen Loss: 2.01942 Disc Loss: 0.362179\n",
      "Gen Loss: 1.59715 Disc Loss: 0.791379\n",
      "Gen Loss: 1.5098 Disc Loss: 0.667507\n",
      "Gen Loss: 1.24138 Disc Loss: 0.971603\n",
      "Gen Loss: 1.06064 Disc Loss: 0.605422\n",
      "Gen Loss: 2.20227 Disc Loss: 0.347035\n",
      "Gen Loss: 1.82622 Disc Loss: 0.556141\n",
      "Gen Loss: 1.35703 Disc Loss: 0.502235\n",
      "Gen Loss: 1.33093 Disc Loss: 0.709633\n",
      "Gen Loss: 1.90648 Disc Loss: 0.513009\n",
      "Gen Loss: 2.66636 Disc Loss: 0.264907\n",
      "Gen Loss: 1.22305 Disc Loss: 0.64628\n",
      "Gen Loss: 2.01062 Disc Loss: 0.68348\n",
      "Gen Loss: 2.25322 Disc Loss: 0.443269\n",
      "Gen Loss: 1.69212 Disc Loss: 0.596722\n",
      "Gen Loss: 1.83076 Disc Loss: 0.446042\n",
      "Gen Loss: 1.96477 Disc Loss: 0.268062\n",
      "Gen Loss: 3.10372 Disc Loss: 0.475583\n",
      "Gen Loss: 2.73551 Disc Loss: 0.557025\n",
      "Gen Loss: 1.21549 Disc Loss: 0.787152\n",
      "Gen Loss: 2.37832 Disc Loss: 0.298066\n",
      "Gen Loss: 2.02923 Disc Loss: 0.484075\n",
      "Gen Loss: 2.28893 Disc Loss: 0.383092\n",
      "Gen Loss: 1.30353 Disc Loss: 0.543158\n",
      "Gen Loss: 1.49405 Disc Loss: 0.50317\n",
      "Gen Loss: 1.64579 Disc Loss: 0.414956\n",
      "Gen Loss: 1.80422 Disc Loss: 0.800265\n",
      "Gen Loss: 1.75318 Disc Loss: 0.466882\n",
      "Gen Loss: 2.46273 Disc Loss: 0.443962\n",
      "Gen Loss: 1.4287 Disc Loss: 1.01338\n",
      "Gen Loss: 1.85033 Disc Loss: 0.507153\n",
      "Gen Loss: 2.21666 Disc Loss: 0.224857\n",
      "Gen Loss: 2.56087 Disc Loss: 0.169984\n",
      "Gen Loss: 2.15685 Disc Loss: 0.605262\n",
      "Gen Loss: 2.05534 Disc Loss: 0.803427\n",
      "Gen Loss: 2.4692 Disc Loss: 0.424443\n",
      "Gen Loss: 2.25375 Disc Loss: 0.226228\n",
      "Gen Loss: 2.3983 Disc Loss: 0.363488\n",
      "Gen Loss: 1.66792 Disc Loss: 0.579427\n",
      "Gen Loss: 1.1301 Disc Loss: 1.08721\n",
      "Gen Loss: 2.06068 Disc Loss: 0.425522\n",
      "Gen Loss: 1.61491 Disc Loss: 0.73697\n",
      "Gen Loss: 2.37149 Disc Loss: 0.244248\n",
      "Gen Loss: 2.47327 Disc Loss: 0.262213\n",
      "Gen Loss: 2.32527 Disc Loss: 0.415203\n",
      "Gen Loss: 2.53234 Disc Loss: 0.235722\n",
      "Gen Loss: 2.20891 Disc Loss: 0.397613\n",
      "Gen Loss: 3.16501 Disc Loss: 0.302832\n",
      "Gen Loss: 2.06695 Disc Loss: 0.341894\n",
      "Gen Loss: 2.11428 Disc Loss: 0.417639\n",
      "Gen Loss: 2.22998 Disc Loss: 0.429847\n",
      "Gen Loss: 1.90705 Disc Loss: 0.37755\n",
      "Gen Loss: 1.81665 Disc Loss: 1.23824\n",
      "Gen Loss: 1.72401 Disc Loss: 0.498891\n",
      "Gen Loss: 1.91672 Disc Loss: 0.93704\n",
      "Gen Loss: 1.96667 Disc Loss: 0.692796\n",
      "Gen Loss: 1.86493 Disc Loss: 0.606904\n",
      "Gen Loss: 2.19155 Disc Loss: 0.569457\n",
      "Gen Loss: 2.61442 Disc Loss: 0.335035\n",
      "Gen Loss: 2.43001 Disc Loss: 0.309285\n",
      "Gen Loss: 2.17095 Disc Loss: 0.36396\n",
      "Gen Loss: 2.41376 Disc Loss: 0.220666\n",
      "Gen Loss: 3.09567 Disc Loss: 0.281291\n",
      "Gen Loss: 2.2191 Disc Loss: 0.274879\n",
      "Gen Loss: 1.66213 Disc Loss: 0.366829\n",
      "Gen Loss: 1.52895 Disc Loss: 0.383783\n",
      "Gen Loss: 2.5062 Disc Loss: 0.203519\n",
      "Gen Loss: 2.8495 Disc Loss: 0.142005\n",
      "Gen Loss: 1.81255 Disc Loss: 0.613105\n",
      "Gen Loss: 1.41238 Disc Loss: 0.455684\n",
      "Gen Loss: 2.35639 Disc Loss: 0.337757\n",
      "Gen Loss: 1.67232 Disc Loss: 0.906422\n",
      "Gen Loss: 2.70701 Disc Loss: 0.445206\n",
      "Gen Loss: 0.46279 Disc Loss: 0.934696\n",
      "Gen Loss: 1.91745 Disc Loss: 0.577569\n",
      "Gen Loss: 2.50464 Disc Loss: 0.298907\n",
      "Gen Loss: 2.65871 Disc Loss: 0.16361\n",
      "Gen Loss: 2.03108 Disc Loss: 0.39008\n",
      "Gen Loss: 0.907 Disc Loss: 0.919776\n",
      "Gen Loss: 1.94078 Disc Loss: 0.367079\n",
      "Gen Loss: 1.37379 Disc Loss: 0.508986\n",
      "Gen Loss: 3.10174 Disc Loss: 0.280522\n",
      "Gen Loss: 1.96714 Disc Loss: 0.322898\n",
      "Gen Loss: 1.28173 Disc Loss: 0.785481\n",
      "Gen Loss: 2.24152 Disc Loss: 0.268289\n",
      "Gen Loss: 2.12179 Disc Loss: 0.324487\n",
      "Gen Loss: 2.12158 Disc Loss: 0.331753\n",
      "Gen Loss: 1.79699 Disc Loss: 0.427213\n",
      "Saved Model\n",
      "Gen Loss: 2.11924 Disc Loss: 0.509605\n",
      "Gen Loss: 2.39063 Disc Loss: 0.199453\n",
      "Gen Loss: 2.25048 Disc Loss: 0.292859\n",
      "Gen Loss: 2.51743 Disc Loss: 0.222119\n",
      "Gen Loss: 1.67179 Disc Loss: 0.363007\n",
      "Gen Loss: 1.4636 Disc Loss: 0.731418\n",
      "Gen Loss: 2.9026 Disc Loss: 0.271006\n",
      "Gen Loss: 2.93433 Disc Loss: 0.149959\n",
      "Gen Loss: 2.25122 Disc Loss: 0.227313\n",
      "Gen Loss: 2.90042 Disc Loss: 0.445092\n",
      "Gen Loss: 3.39606 Disc Loss: 0.201139\n",
      "Gen Loss: 2.09224 Disc Loss: 0.302749\n",
      "Gen Loss: 1.97776 Disc Loss: 0.439829\n",
      "Gen Loss: 2.45556 Disc Loss: 0.341383\n",
      "Gen Loss: 3.67478 Disc Loss: 0.119614\n",
      "Gen Loss: 2.51544 Disc Loss: 0.217913\n",
      "Gen Loss: 2.19534 Disc Loss: 1.12295\n",
      "Gen Loss: 1.62186 Disc Loss: 0.547255\n",
      "Gen Loss: 2.55375 Disc Loss: 0.517538\n",
      "Gen Loss: 3.71791 Disc Loss: 0.317724\n",
      "Gen Loss: 2.76186 Disc Loss: 0.269973\n",
      "Gen Loss: 3.1745 Disc Loss: 0.360515\n",
      "Gen Loss: 2.0753 Disc Loss: 0.450104\n",
      "Gen Loss: 2.14895 Disc Loss: 0.328169\n",
      "Gen Loss: 1.55831 Disc Loss: 0.681525\n",
      "Gen Loss: 2.26349 Disc Loss: 0.200918\n",
      "Gen Loss: 1.18131 Disc Loss: 0.754643\n",
      "Gen Loss: 1.76735 Disc Loss: 0.422115\n",
      "Gen Loss: 2.73414 Disc Loss: 0.294666\n",
      "Gen Loss: 2.52665 Disc Loss: 0.597998\n",
      "Gen Loss: 2.50112 Disc Loss: 0.757885\n",
      "Gen Loss: 1.6622 Disc Loss: 0.45992\n",
      "Gen Loss: 2.5413 Disc Loss: 0.212811\n",
      "Gen Loss: 2.63248 Disc Loss: 0.310061\n",
      "Gen Loss: 1.29214 Disc Loss: 0.761072\n",
      "Gen Loss: 2.80703 Disc Loss: 0.121523\n",
      "Gen Loss: 1.94681 Disc Loss: 0.38365\n",
      "Gen Loss: 2.41228 Disc Loss: 0.544589\n",
      "Gen Loss: 3.2073 Disc Loss: 0.236655\n",
      "Gen Loss: 0.908164 Disc Loss: 0.969502\n",
      "Gen Loss: 1.3941 Disc Loss: 0.499491\n",
      "Gen Loss: 1.83963 Disc Loss: 0.408341\n",
      "Gen Loss: 2.80796 Disc Loss: 0.240604\n",
      "Gen Loss: 0.956747 Disc Loss: 0.454852\n",
      "Gen Loss: 1.5442 Disc Loss: 0.341599\n",
      "Gen Loss: 1.97869 Disc Loss: 0.589998\n",
      "Gen Loss: 2.60141 Disc Loss: 0.330379\n",
      "Gen Loss: 2.15695 Disc Loss: 0.261471\n",
      "Gen Loss: 2.01006 Disc Loss: 0.230663\n",
      "Gen Loss: 2.54574 Disc Loss: 0.316569\n",
      "Gen Loss: 1.38276 Disc Loss: 0.456321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 1.63518 Disc Loss: 0.512847\n",
      "Gen Loss: 1.64857 Disc Loss: 0.876531\n",
      "Gen Loss: 2.06255 Disc Loss: 0.263691\n",
      "Gen Loss: 1.94766 Disc Loss: 0.212303\n",
      "Gen Loss: 2.76047 Disc Loss: 0.277289\n",
      "Gen Loss: 2.58456 Disc Loss: 0.290984\n",
      "Gen Loss: 2.42071 Disc Loss: 0.313112\n",
      "Gen Loss: 2.50669 Disc Loss: 0.252246\n",
      "Gen Loss: 1.76738 Disc Loss: 0.448206\n",
      "Gen Loss: 2.05309 Disc Loss: 0.353318\n",
      "Gen Loss: 1.19555 Disc Loss: 0.68567\n",
      "Gen Loss: 1.73197 Disc Loss: 0.515851\n",
      "Gen Loss: 1.39174 Disc Loss: 0.634054\n",
      "Gen Loss: 2.04131 Disc Loss: 0.171079\n",
      "Gen Loss: 2.5725 Disc Loss: 0.329131\n",
      "Gen Loss: 1.62914 Disc Loss: 0.501752\n",
      "Gen Loss: 2.42886 Disc Loss: 0.270976\n",
      "Gen Loss: 1.51815 Disc Loss: 0.432627\n",
      "Gen Loss: 2.33541 Disc Loss: 0.437812\n",
      "Gen Loss: 1.32107 Disc Loss: 0.688165\n",
      "Gen Loss: 2.35359 Disc Loss: 1.15119\n",
      "Gen Loss: 1.86592 Disc Loss: 0.751316\n",
      "Gen Loss: 2.06555 Disc Loss: 0.435469\n",
      "Gen Loss: 2.237 Disc Loss: 0.299729\n",
      "Gen Loss: 2.10628 Disc Loss: 0.361113\n",
      "Gen Loss: 2.11231 Disc Loss: 0.161764\n",
      "Gen Loss: 1.99017 Disc Loss: 0.299381\n",
      "Gen Loss: 2.42937 Disc Loss: 0.454954\n",
      "Gen Loss: 2.69848 Disc Loss: 0.235726\n",
      "Gen Loss: 2.25772 Disc Loss: 0.369803\n",
      "Gen Loss: 1.80403 Disc Loss: 0.432617\n",
      "Gen Loss: 1.76995 Disc Loss: 0.429827\n",
      "Gen Loss: 1.97439 Disc Loss: 0.559336\n",
      "Gen Loss: 2.89053 Disc Loss: 0.252023\n",
      "Gen Loss: 1.18368 Disc Loss: 0.53185\n",
      "Gen Loss: 1.76868 Disc Loss: 0.530912\n",
      "Gen Loss: 2.21581 Disc Loss: 0.318399\n",
      "Gen Loss: 0.87312 Disc Loss: 0.917213\n",
      "Gen Loss: 1.46841 Disc Loss: 0.950291\n",
      "Gen Loss: 1.41077 Disc Loss: 0.579033\n",
      "Gen Loss: 2.3889 Disc Loss: 0.372893\n",
      "Gen Loss: 1.59877 Disc Loss: 0.322878\n",
      "Gen Loss: 2.80867 Disc Loss: 0.469456\n",
      "Gen Loss: 2.05769 Disc Loss: 0.33304\n",
      "Gen Loss: 1.87779 Disc Loss: 0.456632\n",
      "Gen Loss: 2.01623 Disc Loss: 0.529863\n",
      "Gen Loss: 2.81705 Disc Loss: 0.34201\n",
      "Gen Loss: 1.77937 Disc Loss: 0.916504\n",
      "Gen Loss: 1.3938 Disc Loss: 0.944641\n",
      "Saved Model\n",
      "Gen Loss: 1.85927 Disc Loss: 0.477034\n",
      "Gen Loss: 2.08901 Disc Loss: 0.205483\n",
      "Gen Loss: 2.09084 Disc Loss: 0.487556\n",
      "Gen Loss: 2.36851 Disc Loss: 0.139653\n",
      "Gen Loss: 2.97953 Disc Loss: 0.200135\n",
      "Gen Loss: 1.79351 Disc Loss: 0.429823\n",
      "Gen Loss: 1.21518 Disc Loss: 0.651287\n",
      "Gen Loss: 1.70551 Disc Loss: 0.438095\n",
      "Gen Loss: 1.83071 Disc Loss: 0.913629\n",
      "Gen Loss: 2.03495 Disc Loss: 0.752672\n",
      "Gen Loss: 2.22084 Disc Loss: 0.404189\n",
      "Gen Loss: 1.75181 Disc Loss: 0.464456\n",
      "Gen Loss: 2.62554 Disc Loss: 0.255764\n",
      "Gen Loss: 3.15684 Disc Loss: 0.175512\n",
      "Gen Loss: 2.42034 Disc Loss: 0.348541\n",
      "Gen Loss: 2.3718 Disc Loss: 0.17182\n",
      "Gen Loss: 2.09659 Disc Loss: 0.415045\n",
      "Gen Loss: 1.84071 Disc Loss: 0.813788\n",
      "Gen Loss: 2.20087 Disc Loss: 0.38018\n",
      "Gen Loss: 2.87192 Disc Loss: 0.229607\n",
      "Gen Loss: 1.66074 Disc Loss: 0.431966\n",
      "Gen Loss: 0.530242 Disc Loss: 1.28145\n",
      "Gen Loss: 1.74309 Disc Loss: 0.468377\n",
      "Gen Loss: 1.52946 Disc Loss: 0.952232\n",
      "Gen Loss: 2.47655 Disc Loss: 0.465035\n",
      "Gen Loss: 1.64659 Disc Loss: 0.595154\n",
      "Gen Loss: 2.43439 Disc Loss: 0.380815\n",
      "Gen Loss: 2.21483 Disc Loss: 0.96339\n",
      "Gen Loss: 1.43574 Disc Loss: 0.4392\n",
      "Gen Loss: 2.0378 Disc Loss: 0.603481\n",
      "Gen Loss: 2.11602 Disc Loss: 0.249932\n",
      "Gen Loss: 2.33414 Disc Loss: 0.374298\n",
      "Gen Loss: 2.25751 Disc Loss: 0.208624\n",
      "Gen Loss: 1.89372 Disc Loss: 0.393353\n",
      "Gen Loss: 1.64374 Disc Loss: 0.613942\n",
      "Gen Loss: 1.82815 Disc Loss: 0.539855\n",
      "Gen Loss: 1.83043 Disc Loss: 0.325903\n",
      "Gen Loss: 3.25788 Disc Loss: 0.807591\n",
      "Gen Loss: 2.44235 Disc Loss: 0.26643\n",
      "Gen Loss: 1.61012 Disc Loss: 0.476086\n",
      "Gen Loss: 3.14501 Disc Loss: 0.100475\n",
      "Gen Loss: 2.5118 Disc Loss: 0.159662\n",
      "Gen Loss: 2.75262 Disc Loss: 0.0813754\n",
      "Gen Loss: 2.92474 Disc Loss: 0.178169\n",
      "Gen Loss: 2.72631 Disc Loss: 0.221311\n",
      "Gen Loss: 2.61198 Disc Loss: 0.207231\n",
      "Gen Loss: 2.0244 Disc Loss: 0.476486\n",
      "Gen Loss: 2.39955 Disc Loss: 0.163177\n",
      "Gen Loss: 2.92884 Disc Loss: 0.316343\n",
      "Gen Loss: 2.18955 Disc Loss: 0.433127\n",
      "Gen Loss: 2.78451 Disc Loss: 0.143661\n",
      "Gen Loss: 2.6022 Disc Loss: 0.119854\n",
      "Gen Loss: 2.7998 Disc Loss: 0.164463\n",
      "Gen Loss: 2.83827 Disc Loss: 0.150916\n",
      "Gen Loss: 1.97051 Disc Loss: 0.664135\n",
      "Gen Loss: 1.70416 Disc Loss: 0.362885\n",
      "Gen Loss: 2.31021 Disc Loss: 0.289775\n",
      "Gen Loss: 1.51844 Disc Loss: 0.881722\n",
      "Gen Loss: 1.85338 Disc Loss: 0.2311\n",
      "Gen Loss: 2.06083 Disc Loss: 0.397485\n",
      "Gen Loss: 1.6174 Disc Loss: 0.45811\n",
      "Gen Loss: 1.86585 Disc Loss: 0.613577\n",
      "Gen Loss: 2.31618 Disc Loss: 0.809359\n",
      "Gen Loss: 2.10149 Disc Loss: 0.191639\n",
      "Gen Loss: 2.44262 Disc Loss: 0.348616\n",
      "Gen Loss: 2.02812 Disc Loss: 0.386913\n",
      "Gen Loss: 1.98194 Disc Loss: 0.180878\n",
      "Gen Loss: 2.49675 Disc Loss: 0.212381\n",
      "Gen Loss: 4.97907 Disc Loss: 1.71263\n",
      "Gen Loss: 1.61362 Disc Loss: 0.457569\n",
      "Gen Loss: 1.77285 Disc Loss: 0.310981\n",
      "Gen Loss: 3.06832 Disc Loss: 0.758241\n",
      "Gen Loss: 2.53187 Disc Loss: 0.125807\n",
      "Gen Loss: 1.70481 Disc Loss: 0.574966\n",
      "Gen Loss: 2.61607 Disc Loss: 0.272887\n",
      "Gen Loss: 1.55905 Disc Loss: 0.603365\n",
      "Gen Loss: 2.75506 Disc Loss: 0.342449\n",
      "Gen Loss: 2.04038 Disc Loss: 0.247167\n",
      "Gen Loss: 1.96447 Disc Loss: 0.394775\n",
      "Gen Loss: 2.58569 Disc Loss: 0.556506\n",
      "Gen Loss: 2.1666 Disc Loss: 0.366299\n",
      "Gen Loss: 2.96759 Disc Loss: 0.143388\n",
      "Gen Loss: 2.64466 Disc Loss: 0.482573\n",
      "Gen Loss: 2.5744 Disc Loss: 0.398592\n",
      "Gen Loss: 2.57167 Disc Loss: 0.199865\n",
      "Gen Loss: 2.05593 Disc Loss: 0.267481\n",
      "Gen Loss: 1.65386 Disc Loss: 0.370395\n",
      "Gen Loss: 2.25687 Disc Loss: 0.568826\n",
      "Gen Loss: 2.00986 Disc Loss: 0.762011\n",
      "Gen Loss: 2.05108 Disc Loss: 0.577324\n",
      "Gen Loss: 2.27697 Disc Loss: 0.265239\n",
      "Gen Loss: 3.09065 Disc Loss: 0.156986\n",
      "Gen Loss: 1.23307 Disc Loss: 0.881193\n",
      "Gen Loss: 1.90599 Disc Loss: 0.51886\n",
      "Gen Loss: 2.55421 Disc Loss: 0.273926\n",
      "Gen Loss: 3.20926 Disc Loss: 0.167483\n",
      "Gen Loss: 2.37663 Disc Loss: 0.219523\n",
      "Gen Loss: 2.16161 Disc Loss: 0.308389\n",
      "Gen Loss: 3.38389 Disc Loss: 0.145051\n",
      "Gen Loss: 2.72402 Disc Loss: 0.17424\n",
      "Saved Model\n",
      "Gen Loss: 3.16312 Disc Loss: 0.183404\n",
      "Gen Loss: 2.34518 Disc Loss: 0.326406\n",
      "Gen Loss: 2.01265 Disc Loss: 0.308926\n",
      "Gen Loss: 2.68448 Disc Loss: 0.299778\n",
      "Gen Loss: 1.28262 Disc Loss: 0.846932\n",
      "Gen Loss: 3.87965 Disc Loss: 0.0652303\n",
      "Gen Loss: 2.38829 Disc Loss: 0.468792\n",
      "Gen Loss: 3.5377 Disc Loss: 0.840157\n",
      "Gen Loss: 1.39932 Disc Loss: 0.607362\n",
      "Gen Loss: 0.977975 Disc Loss: 1.60311\n",
      "Gen Loss: 2.46546 Disc Loss: 0.637088\n",
      "Gen Loss: 2.73087 Disc Loss: 0.243677\n",
      "Gen Loss: 2.19132 Disc Loss: 0.484132\n",
      "Gen Loss: 1.48062 Disc Loss: 0.721306\n",
      "Gen Loss: 2.70808 Disc Loss: 0.198769\n",
      "Gen Loss: 2.49941 Disc Loss: 0.207091\n",
      "Gen Loss: 2.9824 Disc Loss: 0.379712\n",
      "Gen Loss: 2.88001 Disc Loss: 0.65624\n",
      "Gen Loss: 2.61964 Disc Loss: 0.166172\n",
      "Gen Loss: 1.7908 Disc Loss: 0.378706\n",
      "Gen Loss: 2.58313 Disc Loss: 0.214489\n",
      "Gen Loss: 3.17718 Disc Loss: 0.804322\n",
      "Gen Loss: 3.47246 Disc Loss: 0.28762\n",
      "Gen Loss: 2.05382 Disc Loss: 0.803587\n",
      "Gen Loss: 2.06765 Disc Loss: 0.249383\n",
      "Gen Loss: 1.70707 Disc Loss: 0.328302\n",
      "Gen Loss: 1.62592 Disc Loss: 0.605607\n",
      "Gen Loss: 1.66004 Disc Loss: 0.40836\n",
      "Gen Loss: 2.15507 Disc Loss: 0.369457\n",
      "Gen Loss: 2.09965 Disc Loss: 0.260976\n",
      "Gen Loss: 1.69299 Disc Loss: 0.41101\n",
      "Gen Loss: 1.87625 Disc Loss: 0.495283\n",
      "Gen Loss: 2.15935 Disc Loss: 0.212287\n",
      "Gen Loss: 2.73722 Disc Loss: 0.26542\n",
      "Gen Loss: 2.15152 Disc Loss: 0.314695\n",
      "Gen Loss: 2.41609 Disc Loss: 0.503236\n",
      "Gen Loss: 3.61581 Disc Loss: 0.14786\n",
      "Gen Loss: 2.47904 Disc Loss: 0.386876\n",
      "Gen Loss: 1.67664 Disc Loss: 0.459973\n",
      "Gen Loss: 1.77006 Disc Loss: 1.207\n",
      "Gen Loss: 2.0066 Disc Loss: 0.508521\n",
      "Gen Loss: 2.149 Disc Loss: 0.349559\n",
      "Gen Loss: 1.42099 Disc Loss: 1.10487\n",
      "Gen Loss: 1.28142 Disc Loss: 0.698938\n",
      "Gen Loss: 1.08447 Disc Loss: 0.588364\n",
      "Gen Loss: 1.63045 Disc Loss: 0.453237\n",
      "Gen Loss: 1.42675 Disc Loss: 0.654626\n",
      "Gen Loss: 1.93536 Disc Loss: 0.443232\n",
      "Gen Loss: 1.34471 Disc Loss: 0.718387\n",
      "Gen Loss: 2.34512 Disc Loss: 0.172427\n",
      "Gen Loss: 1.94625 Disc Loss: 0.56594\n",
      "Gen Loss: 2.06537 Disc Loss: 0.295641\n",
      "Gen Loss: 1.41704 Disc Loss: 0.473983\n",
      "Gen Loss: 1.72536 Disc Loss: 0.514568\n",
      "Gen Loss: 2.42755 Disc Loss: 0.185983\n",
      "Gen Loss: 1.43792 Disc Loss: 0.431552\n",
      "Gen Loss: 1.85447 Disc Loss: 0.462501\n",
      "Gen Loss: 1.71332 Disc Loss: 0.477561\n",
      "Gen Loss: 2.2321 Disc Loss: 0.517982\n",
      "Gen Loss: 1.98846 Disc Loss: 0.673551\n",
      "Gen Loss: 3.19713 Disc Loss: 0.261728\n",
      "Gen Loss: 0.737528 Disc Loss: 0.821618\n",
      "Gen Loss: 1.21469 Disc Loss: 0.498757\n",
      "Gen Loss: 2.26148 Disc Loss: 0.421023\n",
      "Gen Loss: 2.7264 Disc Loss: 0.384612\n",
      "Gen Loss: 2.43084 Disc Loss: 0.209848\n",
      "Gen Loss: 2.65397 Disc Loss: 0.128749\n",
      "Gen Loss: 1.41588 Disc Loss: 0.666059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 1.50505 Disc Loss: 0.233162\n",
      "Gen Loss: 2.41518 Disc Loss: 0.457365\n",
      "Gen Loss: 2.70105 Disc Loss: 0.821257\n",
      "Gen Loss: 2.20656 Disc Loss: 0.271459\n",
      "Gen Loss: 1.84468 Disc Loss: 1.09376\n",
      "Gen Loss: 1.09808 Disc Loss: 0.278311\n",
      "Gen Loss: 2.48201 Disc Loss: 0.0607496\n",
      "Gen Loss: 2.46173 Disc Loss: 0.504972\n",
      "Gen Loss: 2.38348 Disc Loss: 0.323011\n",
      "Gen Loss: 1.83819 Disc Loss: 0.55583\n",
      "Gen Loss: 1.49881 Disc Loss: 0.496647\n",
      "Gen Loss: 2.85894 Disc Loss: 0.251162\n",
      "Gen Loss: 2.1955 Disc Loss: 0.541567\n",
      "Gen Loss: 0.958698 Disc Loss: 0.933514\n",
      "Gen Loss: 2.67118 Disc Loss: 0.203243\n",
      "Gen Loss: 2.62586 Disc Loss: 0.245414\n",
      "Gen Loss: 1.82761 Disc Loss: 0.592739\n",
      "Gen Loss: 2.43876 Disc Loss: 0.317017\n",
      "Gen Loss: 2.35558 Disc Loss: 0.23567\n",
      "Gen Loss: 1.84949 Disc Loss: 0.477985\n",
      "Gen Loss: 1.3358 Disc Loss: 0.40874\n",
      "Gen Loss: 1.77781 Disc Loss: 0.455463\n",
      "Gen Loss: 1.97431 Disc Loss: 0.382409\n",
      "Gen Loss: 2.33616 Disc Loss: 0.301752\n",
      "Gen Loss: 1.82586 Disc Loss: 0.586694\n",
      "Gen Loss: 2.4172 Disc Loss: 0.273042\n",
      "Gen Loss: 2.53952 Disc Loss: 0.794492\n",
      "Gen Loss: 1.02414 Disc Loss: 1.28251\n",
      "Gen Loss: 1.8355 Disc Loss: 0.387986\n",
      "Gen Loss: 2.6142 Disc Loss: 0.169034\n",
      "Gen Loss: 1.82427 Disc Loss: 0.556235\n",
      "Gen Loss: 1.70474 Disc Loss: 0.451911\n",
      "Saved Model\n",
      "Gen Loss: 1.21246 Disc Loss: 0.602742\n",
      "Gen Loss: 1.80779 Disc Loss: 0.503586\n",
      "Gen Loss: 1.29378 Disc Loss: 0.416664\n",
      "Gen Loss: 2.25106 Disc Loss: 0.514107\n",
      "Gen Loss: 1.72738 Disc Loss: 0.546139\n",
      "Gen Loss: 1.51202 Disc Loss: 0.609953\n",
      "Gen Loss: 2.31611 Disc Loss: 0.17106\n",
      "Gen Loss: 3.25543 Disc Loss: 0.151618\n",
      "Gen Loss: 2.65825 Disc Loss: 0.331217\n",
      "Gen Loss: 2.51516 Disc Loss: 0.599143\n",
      "Gen Loss: 1.9925 Disc Loss: 0.717235\n",
      "Gen Loss: 2.18529 Disc Loss: 0.29515\n",
      "Gen Loss: 1.86397 Disc Loss: 0.881302\n",
      "Gen Loss: 1.63163 Disc Loss: 0.362949\n",
      "Gen Loss: 1.97969 Disc Loss: 0.315982\n",
      "Gen Loss: 1.83192 Disc Loss: 0.349052\n",
      "Gen Loss: 1.42683 Disc Loss: 0.318049\n",
      "Gen Loss: 0.778377 Disc Loss: 1.10705\n",
      "Gen Loss: 2.19583 Disc Loss: 0.251319\n",
      "Gen Loss: 1.46866 Disc Loss: 0.842389\n",
      "Gen Loss: 3.03886 Disc Loss: 0.595717\n",
      "Gen Loss: 2.41874 Disc Loss: 0.40325\n",
      "Gen Loss: 2.51177 Disc Loss: 0.193358\n",
      "Gen Loss: 2.63196 Disc Loss: 0.207741\n",
      "Gen Loss: 2.67229 Disc Loss: 0.384854\n",
      "Gen Loss: 2.32575 Disc Loss: 0.675028\n",
      "Gen Loss: 1.6959 Disc Loss: 0.230405\n",
      "Gen Loss: 2.12194 Disc Loss: 0.380759\n",
      "Gen Loss: 1.5872 Disc Loss: 0.534767\n",
      "Gen Loss: 2.87037 Disc Loss: 0.53469\n",
      "Gen Loss: 1.72001 Disc Loss: 0.520107\n",
      "Gen Loss: 1.73516 Disc Loss: 0.375176\n",
      "Gen Loss: 1.91112 Disc Loss: 0.362507\n",
      "Gen Loss: 1.70363 Disc Loss: 0.592265\n",
      "Gen Loss: 2.20456 Disc Loss: 0.619211\n",
      "Gen Loss: 2.41624 Disc Loss: 0.363913\n",
      "Gen Loss: 2.10799 Disc Loss: 0.473089\n",
      "Gen Loss: 1.39286 Disc Loss: 0.821214\n",
      "Gen Loss: 1.93574 Disc Loss: 0.483862\n",
      "Gen Loss: 0.443093 Disc Loss: 0.995943\n",
      "Gen Loss: 2.82971 Disc Loss: 0.182827\n",
      "Gen Loss: 2.1403 Disc Loss: 0.442667\n",
      "Gen Loss: 1.93356 Disc Loss: 0.370504\n",
      "Gen Loss: 2.42781 Disc Loss: 0.200391\n",
      "Gen Loss: 1.52305 Disc Loss: 0.640589\n",
      "Gen Loss: 3.01569 Disc Loss: 0.19649\n",
      "Gen Loss: 2.13359 Disc Loss: 0.402835\n",
      "Gen Loss: 2.48664 Disc Loss: 0.24014\n",
      "Gen Loss: 2.42005 Disc Loss: 0.323724\n",
      "Gen Loss: 1.94119 Disc Loss: 0.360218\n",
      "Gen Loss: 2.38601 Disc Loss: 0.148773\n",
      "Gen Loss: 2.32685 Disc Loss: 0.349205\n",
      "Gen Loss: 1.58112 Disc Loss: 0.334095\n",
      "Gen Loss: 2.17937 Disc Loss: 0.362329\n",
      "Gen Loss: 2.63001 Disc Loss: 0.390565\n",
      "Gen Loss: 2.50135 Disc Loss: 0.349396\n",
      "Gen Loss: 2.0127 Disc Loss: 0.324635\n",
      "Gen Loss: 1.84419 Disc Loss: 0.311202\n",
      "Gen Loss: 1.82685 Disc Loss: 0.383475\n",
      "Gen Loss: 0.94284 Disc Loss: 1.01992\n",
      "Gen Loss: 2.64566 Disc Loss: 0.347383\n",
      "Gen Loss: 1.53087 Disc Loss: 0.534682\n",
      "Gen Loss: 1.65813 Disc Loss: 0.410928\n",
      "Gen Loss: 2.17628 Disc Loss: 0.26638\n",
      "Gen Loss: 1.05816 Disc Loss: 0.615032\n",
      "Gen Loss: 2.29768 Disc Loss: 0.735965\n",
      "Gen Loss: 1.521 Disc Loss: 0.437458\n",
      "Gen Loss: 2.89095 Disc Loss: 0.212895\n",
      "Gen Loss: 1.21357 Disc Loss: 0.555541\n",
      "Gen Loss: 2.62609 Disc Loss: 0.168451\n",
      "Gen Loss: 2.63002 Disc Loss: 0.29983\n",
      "Gen Loss: 2.6662 Disc Loss: 0.262405\n",
      "Gen Loss: 2.48153 Disc Loss: 0.466025\n",
      "Gen Loss: 2.54456 Disc Loss: 0.158847\n",
      "Gen Loss: 2.16385 Disc Loss: 0.431761\n",
      "Gen Loss: 3.86456 Disc Loss: 1.07999\n",
      "Gen Loss: 2.01844 Disc Loss: 0.566365\n",
      "Gen Loss: 1.84463 Disc Loss: 0.536459\n",
      "Gen Loss: 2.66907 Disc Loss: 0.254346\n",
      "Gen Loss: 4.12291 Disc Loss: 0.270749\n",
      "Gen Loss: 2.13686 Disc Loss: 0.404084\n",
      "Gen Loss: 2.08581 Disc Loss: 0.370426\n",
      "Gen Loss: 2.63784 Disc Loss: 0.388297\n",
      "Gen Loss: 3.62874 Disc Loss: 0.259885\n",
      "Gen Loss: 2.3504 Disc Loss: 0.191261\n",
      "Gen Loss: 2.60994 Disc Loss: 0.488594\n",
      "Gen Loss: 1.49808 Disc Loss: 0.706034\n",
      "Gen Loss: 2.48397 Disc Loss: 0.363725\n",
      "Gen Loss: 2.38864 Disc Loss: 0.147752\n",
      "Gen Loss: 4.0137 Disc Loss: 0.0937944\n",
      "Gen Loss: 3.4222 Disc Loss: 0.104311\n",
      "Gen Loss: 3.23426 Disc Loss: 0.143676\n",
      "Gen Loss: 2.72929 Disc Loss: 0.320647\n",
      "Gen Loss: 2.47752 Disc Loss: 0.418707\n",
      "Gen Loss: 2.62622 Disc Loss: 0.471731\n",
      "Gen Loss: 3.80438 Disc Loss: 0.24016\n",
      "Gen Loss: 1.88043 Disc Loss: 0.527758\n",
      "Gen Loss: 1.46768 Disc Loss: 0.21395\n",
      "Gen Loss: 2.76177 Disc Loss: 0.237244\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16 #Size of image batch to apply at each iteration.\n",
    "iterations = 10000 #Total number of iterations to use.\n",
    "sample_directory = './figs' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to save trained model to.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        zs = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
    "        xs,_ = mnist.train.next_batch(batch_size) #Draw a sample batch from MNIST dataset.\n",
    "        xs = (np.reshape(xs,[batch_size,28,28,1]) - 0.5) * 2.0 #Transform it to be between -1 and 1\n",
    "        xs = np.lib.pad(xs, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) #Pad the images so the are 32x32\n",
    "        _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs}) #Update the discriminator\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs}) #Update the generator, twice for good measure.\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs})\n",
    "        if i % 10 == 0:\n",
    "            print (\"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss))\n",
    "            z2 = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate another z batch\n",
    "            newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "            #Save sample generator images for viewing training progress.\n",
    "            #save_images(np.reshape(newZ[0:36],[36,32,32]),[6,6],sample_directory+'/fig'+str(i)+'.png')\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
    "            print (\"Saved Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a trained network\n",
    "Once we have a trained model saved, we may want to use it to generate new images, and explore the representation it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/model-9000.cptk\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f5d7c481491c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmy_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray_r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Volumes/Second_HD/anaconda/envs/tensorflow/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3155\u001b[0m                         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3156\u001b[0m                         \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3157\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   3158\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3159\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/Second_HD/anaconda/envs/tensorflow/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1897\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/Second_HD/anaconda/envs/tensorflow/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5122\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5124\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5125\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/Second_HD/anaconda/envs/tensorflow/lib/python3.5/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    598\u001b[0m         if (self._A.ndim not in (2, 3) or\n\u001b[1;32m    599\u001b[0m                 (self._A.ndim == 3 and self._A.shape[-1] not in (3, 4))):\n\u001b[0;32m--> 600\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_imcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    }
   ],
   "source": [
    "sample_directory = './figs' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to load trained model from.\n",
    "batch_size_sample = 1\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    #Reload the model.\n",
    "    print ('Loading Model...')\n",
    "    ckpt = tf.train.get_checkpoint_state(model_directory)\n",
    "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    zs = np.random.uniform(-1.0,1.0,size=[1,z_size]).astype(np.float32) #Generate a random z batch\n",
    "    newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
    "    if not os.path.exists(sample_directory):\n",
    "        os.makedirs(sample_directory)\n",
    "    ## save_images(np.reshape(newZ[0:batch_size_sample],[36,32,32]),[6,6],sample_directory+'/fig'+str(i)+'.png')\n",
    "    \n",
    "    my_i = newZ.squeeze()\n",
    "    plt.imshow(my_i[1,:,:], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11578f2e8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJErhF0mUwi+SqP8DdYlwS/9Ue/oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114137208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(my_i[8,:,:], cmap='gray_r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

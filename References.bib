@article{Lotter2016,
abstract = {The ability to predict future states of the environment is a central pillar of intelligence. At its core, effective prediction requires an internal model of the world and an understanding of the rules by which the world changes. Here, we explore the internal models developed by deep neural networks trained using a loss based on predicting future frames in synthetic video sequences, using a CNN-LSTM-deCNN framework. We first show that this architecture can achieve excellent performance in visual sequence prediction tasks, including state-of-the-art performance in a standard 'bouncing balls' dataset (Sutskever et al., 2009). Using a weighted mean-squared error and adversarial loss (Goodfellow et al., 2014), the same architecture successfully extrapolates out-of-the-plane rotations of computer-generated faces. Furthermore, despite being trained end-to-end to predict only pixel-level information, our Predictive Generative Networks learn a representation of the latent structure of the underlying three-dimensional objects themselves. Importantly, we find that this representation is naturally tolerant to object transformations, and generalizes well to new tasks, such as classification of static images. Similar models trained solely with a reconstruction loss fail to generalize as effectively. We argue that prediction can serve as a powerful unsupervised loss for learning rich internal representations of high-level object features.},
archivePrefix = {arXiv},
arxivId = {1511.06380},
author = {Lotter, William and Kreiman, Gabriel and Cox, David},
eprint = {1511.06380},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Lotter, Kreiman, Cox - Unknown - UNSUPERVISED LEARNING OF VISUAL STRUCTURE USING PREDICTIVE GENERATIVE NETWORKS.pdf:pdf},
journal = {ICLR 2016},
month = {nov},
title = {{Unsupervised Learning of Visual Structure using Predictive Generative Networks}},
url = {http://arxiv.org/abs/1511.06380},
year = {2016}
}
@article{Ledig2016,
abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
archivePrefix = {arXiv},
arxivId = {1609.04802},
author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
eprint = {1609.04802},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Ledig et al. - 2016 - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.pdf:pdf},
journal = {arXiv:1609.04802},
month = {sep},
pages = {19},
title = {{Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}},
url = {https://arxiv.org/pdf/1609.04802.pdf http://arxiv.org/abs/1609.04802},
year = {2016}
}
@article{White2016,
abstract = {—We introduce several techniques for effectively sampling and visualizing the latent spaces of generative models. Replacing linear interpolation with spherical linear interpolation (slerp) prevents diverging from a model's prior distribution and produces sharper samples. J-Diagrams and MINE grids are introduced as visualizations of manifolds created by analogies and nearest neighbors. We demonstrate two new techniques for deriving attribute vectors: bias-corrected vectors with data replication and synthetic vectors with data augmentation. Most techniques are intended to be independent of model type and examples are shown on both Variational Autoencoders and Generative Adversarial Networks.},
archivePrefix = {arXiv},
arxivId = {1609.04468},
author = {White, Tom},
eprint = {1609.04468},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/White - 2016 - Sampling Generative Networks.pdf:pdf},
journal = {Neural Information Processing Systems},
keywords = {GAN,Manifold,Sampling,VAE,—Generative},
number = {Nips},
pages = {1--11},
title = {{Sampling Generative Networks}},
url = {https://arxiv.org/pdf/1609.04468.pdf http://arxiv.org/abs/1609.04468},
year = {2016}
}
@article{Isola2016,
abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
archivePrefix = {arXiv},
arxivId = {1611.07004},
author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
doi = {arXiv:1611.07004},
eprint = {1611.07004},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Isola et al. - Unknown - Image-to-Image Translation with Conditional Adversarial Networks.pdf:pdf},
journal = {arXiv},
pages = {16},
title = {{Image-to-Image Translation with Conditional Adversarial Networks}},
url = {https://arxiv.org/pdf/1611.07004.pdf http://arxiv.org/abs/1611.07004},
year = {2016}
}
@article{Springenberg2015,
abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
archivePrefix = {arXiv},
arxivId = {1412.6806},
author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
doi = {10.1163/_q3_SIM_00374},
eprint = {1412.6806},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Springenberg et al. - Unknown - STRIVING FOR SIMPLICITY THE ALL CONVOLUTIONAL NET.pdf:pdf},
isbn = {9781600066634},
issn = {02548704},
journal = {Iclr},
pages = {1--14},
pmid = {974},
title = {{Striving for Simplicity: The All Convolutional Net}},
url = {https://arxiv.org/pdf/1412.6806.pdf http://arxiv.org/abs/1412.6806},
year = {2015}
}
@article{Nowozin2016,
abstract = {Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.},
archivePrefix = {arXiv},
arxivId = {1606.00709},
author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
eprint = {1606.00709},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Nowozin, Cseke, Tomioka - Unknown - f -GAN Training Generative Neural Samplers using Variational Divergence Minimization.pdf:pdf},
issn = {10495258},
journal = {arXiv},
number = {1},
pages = {17},
title = {{f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization}},
url = {https://arxiv.org/pdf/1606.00709.pdf http://arxiv.org/abs/1606.00709},
volume = {2},
year = {2016}
}
@article{Bengio2014,
abstract = {Recent work showed that denoising auto-encoders can be interpreted as generative models. We generalize these results to arbitrary parametrizations that learn to reconstruct their input and where noise is injected, not just in input, but also in intermediate computations. We show that under reasonable assumptions (the parametrization is rich enough to provide a consistent estimator, and it prevents the learner from just copying its input in output and producing a dirac output distribution), such models are consistent estimators of the data generating distributions, and that they define the estimated distribution through a Markov chain that consists at each step in re-injecting sampled reconstructions as a sequence of inputs into the unfolded computational graph. As a consequence, one can define deep architectures similar to deep Boltzmann machines in that units are stochastic, that the model can learn to generate a distribution similar to its training distribution, that it can easily handle missing inputs, but without the troubling problem of intractable partition function and intractable inference as stumbling blocks for both training and using these models. In particular, we argue that if the underlying latent variables of a graphical model form a highly multi-modal posterior (given the input), none of the currently known training methods can appropriately deal with this multi-modality (when the number modes is much greater than the number of MCMC samples one is willing to perform, and when the structure of the posterior cannot be easily approximated by some tractable variational approximation). In contrast, the proposed models can simply be trained by back-propagating the reconstruction error (seen as log-likelihood of reconstruction) into the parameters, benefiting from the power and ease of training recently demonstrated for deep supervised networks with dropout noise.},
archivePrefix = {arXiv},
arxivId = {1306.1091},
author = {Bengio, Yoshua and Thibodeau-Laufer, {\'{E}}ric and Alain, Guillaume and Yosinski, Jason},
eprint = {1306.1091},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Bengio et al. - 2014 - Deep Generative Stochastic Networks Trainable by Backprop.pdf:pdf},
isbn = {9781634393973},
issn = {1938-7228},
journal = {International Conference on Machine Learning},
keywords = {deep learning},
pages = {226--234},
title = {{Deep Generative Stochastic Networks Trainable by Backprop}},
url = {https://arxiv.org/pdf/1306.1091.pdf http://arxiv.org/abs/1306.1091},
year = {2014}
}
@article{Kingma2014,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
doi = {10.1051/0004-6361/201527329},
eprint = {1312.6114},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Welling - Unknown - Auto-Encoding Variational Bayes.pdf:pdf},
isbn = {1312.6114v10},
issn = {1312.6114v10},
journal = {Iclr},
number = {Ml},
pages = {1--14},
pmid = {23459267},
title = {{Auto-Encoding Variational Bayes}},
url = {https://arxiv.org/pdf/1312.6114.pdf http://arxiv.org/abs/1312.6114},
year = {2014}
}
@article{Lotter2016a,
abstract = {The ability to predict future states of the environment is a central pillar of intelligence. At its core, effective prediction requires an internal model of the world and an understanding of the rules by which the world changes. Here, we explore the internal models developed by deep neural networks trained using a loss based on predicting future frames in synthetic video sequences, using a CNN-LSTM-deCNN framework. We first show that this architecture can achieve excellent performance in visual sequence prediction tasks, including state-of-the-art performance in a standard 'bouncing balls' dataset (Sutskever et al., 2009). Using a weighted mean-squared error and adversarial loss (Goodfellow et al., 2014), the same architecture successfully extrapolates out-of-the-plane rotations of computer-generated faces. Furthermore, despite being trained end-to-end to predict only pixel-level information, our Predictive Generative Networks learn a representation of the latent structure of the underlying three-dimensional objects themselves. Importantly, we find that this representation is naturally tolerant to object transformations, and generalizes well to new tasks, such as classification of static images. Similar models trained solely with a reconstruction loss fail to generalize as effectively. We argue that prediction can serve as a powerful unsupervised loss for learning rich internal representations of high-level object features.},
archivePrefix = {arXiv},
arxivId = {1511.06380},
author = {Lotter, William and Kreiman, Gabriel and Cox, David},
eprint = {1511.06380},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Ba - Unknown - ADAM A METHOD FOR STOCHASTIC OPTIMIZATION(3).pdf:pdf},
journal = {ICLR},
month = {nov},
title = {{Unsupervised Learning of Visual Structure using Predictive Generative Networks}},
url = {https://arxiv.org/pdf/1412.6980.pdf http://arxiv.org/abs/1511.06380},
year = {2016}
}
@article{Oord2016,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predic-tive distribution for each audio sample conditioned on all previous ones; nonethe-less we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
archivePrefix = {arXiv},
arxivId = {arXiv:1609.03499v2},
author = {van den Oord, Aaron and Simonyan, Karen and Kalchbrenner, Nal and Dieleman, Sander and Vinyals, Oriol and Senior, Andrew and Zen, Heiga and Graves, Alex and Kavukcuoglu, Koray},
doi = {10.1109/ICASSP.2009.4960364},
eprint = {arXiv:1609.03499v2},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Van Den Oord et al. - Unknown - WAVENET A GENERATIVE MODEL FOR RAW AUDIO.pdf:pdf},
isbn = {9783901882760},
issn = {0899-7667},
pages = {1--15},
pmid = {18785855},
title = {{WAVENET: A GENERATIVE MODEL FOR RAW AUDIO}},
url = {https://arxiv.org/pdf/1609.03499.pdf},
year = {2016}
}
@inproceedings{Zhu2016,
abstract = {Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to "fall off" the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles.},
archivePrefix = {arXiv},
arxivId = {1609.03552},
author = {Zhu, Jun Yan and Kr{\"{a}}henb{\"{u}}hl, Philipp and Shechtman, Eli and Efros, Alexei A},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46454-1_36},
eprint = {1609.03552},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - Unknown - Generative Visual Manipulation on the Natural Image Manifold.pdf:pdf},
isbn = {9783319464534},
issn = {16113349},
pages = {597--613},
pmid = {4520227},
title = {{Generative visual manipulation on the natural image manifold}},
url = {https://arxiv.org/pdf/1609.03552.pdf},
volume = {9909 LNCS},
year = {2016}
}
@article{Wang2016,
abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
archivePrefix = {arXiv},
arxivId = {1604.07379},
author = {Wang, Xiaolong Xiaogang and Gupta, Abhinav and Xue, Tianfan and Wu, Jiajun and Bouman, Katherine L. and Freeman, William T. and Phadtare, Amruta and Bahmani, Anu and Shah, Anand and Pietrobon, Ricardo and Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter and Goodfellow, Ian and Liu, Ming-Yu and Tuzel, Oncel and Perarnau, Guim and Weijer, Joost Van De and Raducanu, Bogdan and {\'{A}}lvarez, Jose M and Csiro, Data and Hall, Neil and Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak and Walker, Jacob and Doersch, Carl and Gupta, Abhinav and Hebert, Martial and Pathak, Deepak and Donahue, Jeff and Efros, Alexei A. and Im, Daniel Jiwoong and Kim, Chris Dongjoo and Jiang, Hui and Memisevic, Roland and Eccv, Anonymous and Gregor, K and Danihelka, I and Graves, A and Wierstra, D and Nguyen, Anh and Yosinski, Jason and Bengio, Yoshua and Dosovitskiy, Alexey and Clune, Jeff and Zhao, Junbo and Mathieu, Michael and Lecun, Yann and Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Huang, Xun Xiaolei and Wang, Xiaolong Xiaogang and Metaxas, Dimitris and Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A. and Huang, Xun Xiaolei and Li, Yixuan and Poursaeed, Omid and Hopcroft, John and Belongie, Serge},
doi = {10.1109/CVPR.2016.278},
eprint = {1604.07379},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen et al. - 2016 - Plug {\&} Play Generative Networks Conditional Iterative Generation of Images in Latent Space.pdf:pdf},
isbn = {1581138285},
issn = {10636919},
journal = {Cvpr 2016},
keywords = {Action forecasting,Generative models,Scene understanding,Scientific Writing,Variational autoencoders},
month = {nov},
number = {2014},
pages = {424},
pmid = {25315513},
title = {{Plug {\&} Play Generative Networks: Conditional Iterative Generation of Images in Latent Space}},
url = {https://arxiv.org/pdf/1612.00005.pdf http://arxiv.org/abs/1612.00005 https://arxiv.org/pdf/1612.00005.pdf{\%}5Cnhttp://arxiv.org/abs/1611.07004{\%}0Ahttp://arxiv.org/abs/1612.03242{\%}0Ahttp://arxiv.org/abs/1502.04623{\%}0Ahttp://arxiv.org/abs/1602.05110{\%}0Ahttp://genomebiology.com/2014/15/7/424{\%}0Ahttp://arxiv.org/abs/1606.07536{\%}0Ahttp://},
volume = {9911 LNCS},
year = {2016}
}
@article{Chen2016,
abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
archivePrefix = {arXiv},
arxivId = {1606.03657},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
doi = {10.1007/978-3-319-16817-3},
eprint = {1606.03657},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - Unknown - InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:pdf},
isbn = {978-3-319-16816-6},
issn = {978-3-319-16807-4},
journal = {arXiv:1606.03657 [cs.LG]},
number = {Nips},
pages = {1--14},
pmid = {23459267},
title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
url = {http://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf http://arxiv.org/abs/1606.03657},
year = {2016}
}
@article{Salimans,
abstract = {PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connec-tions to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demon-strate the usefulness of these modifications.},
author = {Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Salimans et al. - Unknown - PIXELCNN IMPROVING THE PIXELCNN WITH DISCRETIZED LOGISTIC MIXTURE LIKELIHOOD AND OTHER MODIFICATIONS.pdf:pdf},
title = {{PIXELCNN++: IMPROVING THE PIXELCNN WITH DISCRETIZED LOGISTIC MIXTURE LIKELIHOOD AND OTHER MODIFICATIONS}},
url = {https://arxiv.org/pdf/1701.05517.pdf}
}
@article{Salimans2016,
abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classifica-tion on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a hu-man error rate of 21.3{\%}. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable fea-tures of ImageNet classes.},
archivePrefix = {arXiv},
arxivId = {1606.03498},
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
doi = {arXiv:1504.01391},
eprint = {1606.03498},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Salimans et al. - Unknown - Improved Techniques for Training GANs.pdf:pdf},
isbn = {0924-6495},
issn = {09246495},
journal = {Nips},
pages = {1--10},
pmid = {23259955},
title = {{Improved Techniques for Training GANs}},
url = {http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf},
year = {2016}
}
@article{Reed2017,
abstract = {PixelCNN achieves state-of-the-art results in density estimation for natural images. Although training is fast, inference is costly, requiring one network evaluation per pixel; O(N) for N pixels. This can be sped up by caching activations, but still involves generating each pixel sequentially. In this work, we propose a parallelized PixelCNN that allows more efficient inference by modeling certain pixel groups as conditionally independent. Our new PixelCNN model achieves competitive density estimation and orders of magnitude speedup - O(log N) sampling instead of O(N) - enabling the practical generation of 512x512 images. We evaluate the model on class-conditional image generation, text-to-image synthesis, and action-conditional video generation, showing that our model achieves the best results among non-pixel-autoregressive density models that allow efficient sampling.},
archivePrefix = {arXiv},
arxivId = {1703.03664},
author = {Reed, Scott and van den Oord, A{\"{a}}ron and Kalchbrenner, Nal and Colmenarejo, Sergio G{\'{o}}mez and Wang, Ziyu and Belov, Dan and de Freitas, Nando},
eprint = {1703.03664},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Reed et al. - Unknown - Parallel Multiscale Autoregressive Density Estimation.pdf:pdf},
journal = {arXiv},
month = {mar},
title = {{Parallel Multiscale Autoregressive Density Estimation}},
url = {https://arxiv.org/pdf/1703.03664.pdf http://arxiv.org/abs/1703.03664},
year = {2017}
}
@article{Reed2016,
abstract = {Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neu-ral network architectures have been developed to learn discriminative text feature representa-tions. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to gen-erate highly compelling images of specific cat-egories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image model-ing, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.},
archivePrefix = {arXiv},
arxivId = {1605.05396},
author = {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
doi = {10.1017/CBO9781107415324.004},
eprint = {1605.05396},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Reed et al. - Unknown - Generative Adversarial Text to Image Synthesis.pdf:pdf},
isbn = {9781510829008},
issn = {1550-5499},
journal = {Icml},
pages = {1060--1069},
pmid = {25246403},
title = {{Generative Adversarial Text to Image Synthesis}},
url = {http://proceedings.mlr.press/v48/reed16.pdf},
year = {2016}
}
@article{Metz2016,
abstract = {We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.},
archivePrefix = {arXiv},
arxivId = {1611.02163},
author = {Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
eprint = {1611.02163},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Metz et al. - Unknown - UNROLLED GENERATIVE ADVERSARIAL NETWORKS.pdf:pdf},
month = {nov},
title = {{Unrolled Generative Adversarial Networks}},
url = {https://arxiv.org/pdf/1611.02163.pdf http://arxiv.org/abs/1611.02163},
year = {2016}
}
@article{Makhzani2015,
abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
archivePrefix = {arXiv},
arxivId = {1511.05644},
author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
eprint = {1511.05644},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Makhzani et al. - Unknown - Adversarial Autoencoders.pdf:pdf},
month = {nov},
title = {{Adversarial Autoencoders}},
url = {https://arxiv.org/pdf/1511.05644.pdf http://arxiv.org/abs/1511.05644},
year = {2015}
}
@article{Salimans2016a,
abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classifica-tion on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a hu-man error rate of 21.3{\%}. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable fea-tures of ImageNet classes.},
archivePrefix = {arXiv},
arxivId = {1606.03498},
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
doi = {arXiv:1504.01391},
eprint = {1606.03498},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Salimans et al. - Unknown - Improved Techniques for Training GANs.pdf:pdf},
isbn = {0924-6495},
issn = {09246495},
journal = {Nips},
pages = {1--10},
pmid = {23259955},
title = {{Improved Techniques for Training GANs}},
url = {http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf},
year = {2016}
}
@article{Abadi2015,
abstract = {TensorFlow [1] is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Martin and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vinyals, Oriol and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
doi = {10.1038/nn.3331},
eprint = {1603.04467},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Abadi et al. - Unknown - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
isbn = {0010-0277},
issn = {0270-6474},
journal = {None},
number = {212},
pages = {19},
pmid = {16411492},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {https://arxiv.org/pdf/1603.04467.pdf http://download.tensorflow.org/paper/whitepaper2015.pdf},
volume = {1},
year = {2015}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
eprint = {1406.2661},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
month = {jun},
pages = {2672--2680},
title = {{Generative Adversarial Networks}},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf http://arxiv.org/abs/1406.2661},
year = {2014}
}
@article{Radford2015,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
eprint = {1511.06434},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Radford, Metz, Chintala - 2015 - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.pdf:pdf},
month = {nov},
pages = {A64},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {https://arxiv.org/pdf/1511.06434.pdf{\"{i}}¼‰ http://arxiv.org/abs/1511.06434},
volume = {587},
year = {2015}
}
@inproceedings{Odena2016,
abstract = {We extend Generative Adversarial Networks (GANs) to the semi-supervised context by forcing the discriminator network to output class labels. We train a generative model G and a discriminator D on a dataset with inputs belonging to one of N classes. At training time, D is made to predict which of N+1 classes the input belongs to, where an extra class is added to correspond to the outputs of G. We show that this method can be used to create a more data-efficient classifier and that it allows for generating higher quality samples than a regular GAN.},
archivePrefix = {arXiv},
arxivId = {1606.01583},
author = {Odena, Augustus},
booktitle = {ICML},
doi = {arXiv:1504.01391},
eprint = {1606.01583},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Odena - Unknown - Semi-Supervised Learning with Generative Adversarial Networks.pdf:pdf},
isbn = {1406.2661},
issn = {0253-0465},
keywords = {GAN,Semi-Supervised Learning},
pages = {1--3},
pmid = {15040217},
title = {{Semi-Supervised Learning with Generative Adversarial Networks}},
url = {https://arxiv.org/pdf/1606.01583.pdf http://arxiv.org/abs/1606.01583},
year = {2016}
}
@article{Goodfellow2016,
abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
archivePrefix = {arXiv},
arxivId = {1701.00160},
author = {Goodfellow, Ian},
doi = {10.1001/jamainternmed.2016.8245},
eprint = {1701.00160},
file = {:Users/jeremyhartmann/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow - 2016 - NIPS 2016 Tutorial Generative Adversarial Networks.pdf:pdf},
isbn = {1581138285},
issn = {0253-0465},
pmid = {15040217},
title = {{NIPS 2016 Tutorial: Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1701.00160},
year = {2016}
}
